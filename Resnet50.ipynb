{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650e337b-eb1b-4d14-944b-2fd09ce49de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19f62ffccb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "torch.manual_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752fde4-fbd0-4ecb-9cb5-f51bf79db852",
   "metadata": {},
   "source": [
    "# Import Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9570f3a7-d66e-47ba-b95e-c37753b9ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir_p1 = \"C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_images_part_1\"\n",
    "image_dir_p2 = \"C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_images_part_2\"\n",
    "image_dir_p3 = \"C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_segmentations_lesion_tschandl/HAM10000_segmentations_lesion_tschandl\"\n",
    "\n",
    "image_list = []\n",
    "name_list = []\n",
    "\n",
    "seg_list = []\n",
    "segname_list = []\n",
    "#Load Images(450, 600, 3)\n",
    "for images in os.listdir(image_dir_p1):\n",
    "    image = os.path.join(image_dir_p1, images)\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    name_list.append(images[:-4])\n",
    "    image_list.append(image)\n",
    "\n",
    "# #Load Images\n",
    "for images in os.listdir(image_dir_p2):\n",
    "    image = os.path.join(image_dir_p2, images)\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "\n",
    "    name_list.append(images[:-4])\n",
    "    image_list.append(image)\n",
    "\n",
    "\n",
    "#Load Segmentation Images\n",
    "for images in os.listdir(image_dir_p3):\n",
    "    image = os.path.join(image_dir_p3, images)\n",
    "    image = cv2.imread(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    \n",
    "    segname_list.append(images[:-17])\n",
    "    seg_list.append(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734dffb1-1477-4a6d-9efb-929a9f43c39e",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e12f22ee-3228-45f1-8158-9b622f39ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Melanoma_Dataset(Dataset):\n",
    "    def __init__(self, df, column_data, column_y, weights, device):\n",
    "        super(Melanoma_Dataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.column_data = column_data\n",
    "        self.column_y = column_y\n",
    "        self.weights= weights\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''Return rows of panda columns'''\n",
    "\n",
    "        cur_img = self.df.loc[idx, self.column_data]\n",
    "        cur_img = torch.tensor(cur_img.tolist(), dtype=torch.float32)\n",
    "        cur_img = cur_img.to(self.device)\n",
    "\n",
    "        temp_list = np.zeros(7)\n",
    "        temp_list[int(self.column_y[idx])] = 1\n",
    "        cur_target = torch.tensor(temp_list, dtype=torch.float32)\n",
    "        cur_target = cur_target.to(self.device)\n",
    "\n",
    "        cur_weights = self.df.loc[idx, self.weights]\n",
    "        cur_weights = torch.tensor(cur_weights.tolist(), dtype=torch.float32)\n",
    "        cur_weights = cur_weights.to(self.device)\n",
    "\n",
    "\n",
    "        return cur_img, cur_target, cur_weights\n",
    "\n",
    "def transformToTensor(X):\n",
    "    ret_list = []\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor()\n",
    "              ])\n",
    "    for image in X:\n",
    "      ret_list.append(transform(np.array(image)))\n",
    "\n",
    "    return torch.tensor(np.array(ret_list), dtype=torch.float32, device =\"cuda\")\n",
    "\n",
    "\n",
    "def weight_accuracy(predicted_y, true_y, weight):\n",
    "    pred_y, tar_y, w = np.array(predicted_y), np.array(true_y), np.array(weight)\n",
    "    return np.sum((pred_y == tar_y) * w.reshape(w.size)) / np.sum(w)\n",
    "\n",
    "def num_accuracy(predicted_y, true_y):\n",
    "    pred_y, tar_y = np.array(predicted_y), np.array(true_y)\n",
    "    return np.sum((pred_y == tar_y)) / len(pred_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726748a-984c-4960-80ec-47046d00aff1",
   "metadata": {},
   "source": [
    "# Process images/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71940939-a33b-4cfa-ba19-fd950381b8a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m image_metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m image_metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(image_metadata)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_metadata\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#(10015)\u001b[39;00m\n",
      "File \u001b[1;32mZ:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mZ:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mZ:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mZ:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mZ:\\Anaconda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_metadata'"
     ]
    }
   ],
   "source": [
    "image_metadata = \"C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/HAM10000_metadata\"\n",
    "image_metadata = pd.read_csv(image_metadata)\n",
    "print(image_metadata.shape)\n",
    "\n",
    "#(10015)\n",
    "y = image_metadata[[\"image_id\", \"dx\"]]\n",
    "\n",
    "#(10015, 450, 600, 3)\n",
    "image_dataframe = pd.DataFrame({'image_id': name_list, 'data':image_list})\n",
    "seg_dataframe = pd.DataFrame({'image_id': segname_list, 'segdata': seg_list})\n",
    "\n",
    "#Clear memory\n",
    "name_list =[]\n",
    "image_list =[]\n",
    "segname_list =[]\n",
    "seg_list = []\n",
    "\n",
    "#Weights\n",
    "temp = pd.Categorical(image_metadata[\"dx\"])\n",
    "values = temp.value_counts()\n",
    "\n",
    "weight_list = []\n",
    "label_list = []\n",
    "for label in temp.categories:\n",
    "    label_list.append(label)\n",
    "    weight_list.append(10015/(7*values[label]))\n",
    "weighted = pd.DataFrame({'dx': label_list, 'weights':weight_list})\n",
    "print(weighted)\n",
    "\n",
    "#Merge dataframes\n",
    "fin_dataframe = pd.merge(image_dataframe, y)\n",
    "fin_dataframe = pd.merge(fin_dataframe, seg_dataframe)\n",
    "fin_dataframe = pd.merge(fin_dataframe, weighted)\n",
    "print(fin_dataframe)\n",
    "\n",
    "#Mask images\n",
    "for index, row in fin_dataframe.iterrows():\n",
    "    fin_dataframe.at[index, 'data'] = cv2.bitwise_and(row['data'], row['data'], mask=row['segdata'])\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(fin_dataframe.iloc[0]['data'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c06f37-39ad-489d-8edf-25b0a5c08aa3",
   "metadata": {},
   "source": [
    "# Load into Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "a0a13349-c9b2-4fbd-814c-2ac32e62c6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10015, 5)\n",
      "All weights: tensor([ 0.2134,  1.2855,  1.3018,  2.7835,  4.3753, 10.0755, 12.4410])\n",
      "(8012, 5)\n",
      "(2003, 5)\n",
      "['df', 'nv', 'bkl', 'mel', 'vasc', 'akiec', 'bcc']\n",
      "Categories (7, object): ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
      "torch.Size([128, 224, 224, 3]) torch.Size([128, 7]) torch.Size([128])\n",
      "16\n",
      "torch.Size([128, 224, 224, 3]) torch.Size([128, 7]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "#Shuffle Data\n",
    "fin_dataframe = fin_dataframe.sample(frac=1,random_state=5).reset_index(drop=True)\n",
    "print(fin_dataframe.shape)\n",
    "all_weights = torch.tensor(np.unique(fin_dataframe['weights']), dtype=torch.float32)\n",
    "print(\"All weights:\", all_weights)\n",
    "\n",
    "#80:20 Split\n",
    "train_dataset = fin_dataframe.iloc[:int(.8*len(fin_dataframe ))]\n",
    "valid_dataset = fin_dataframe.iloc[int(.8*len(fin_dataframe )):].reset_index(drop=True)\n",
    "\n",
    "print(train_dataset.shape)\n",
    "print(valid_dataset.shape)\n",
    "print(pd.Categorical(valid_dataset[\"dx\"]).unique())\n",
    "\n",
    "train_dataset = Melanoma_Dataset(train_dataset, \"data\", pd.Categorical(train_dataset[\"dx\"]).codes, \"weights\")\n",
    "valid_dataset = Melanoma_Dataset(valid_dataset, \"data\", pd.Categorical(valid_dataset[\"dx\"]).codes, \"weights\")\n",
    "\n",
    "train_dataset = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "valid_dataset = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=128, shuffle=False)\n",
    "X,Y, W = next(iter(train_dataset))\n",
    "print(X.shape, Y.shape, W.shape)\n",
    "\n",
    "print(len(valid_dataset))\n",
    "X,Y, W = next(iter(valid_dataset))\n",
    "print(X.shape, Y.shape, W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d82b14-abeb-4477-85fd-f7f0422a6d48",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "840f6c86-5ed1-4b46-9493-a6a10046b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50(weights='DEFAULT')\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, 7)\n",
    "\n",
    "#Freeze the pre-trained layers\n",
    "for parameter in resnet50.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet50.parameters(), lr=0.0001)\n",
    "\n",
    "#Unfreeze last few parameters\n",
    "for parameter in resnet50.layer4.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in resnet50.fc.parameters():\n",
    "    parameter.requires_grad = True\n",
    "\n",
    "conf_list = []\n",
    "precise_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "weightacc_list = []\n",
    "numacc_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42285d52-e436-42a4-924e-c269356291fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[1;32m----> 2\u001b[0m total_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      4\u001b[0m     resnet50\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "num_epochs = 15\n",
    "total_step = len(train_dataset)\n",
    "for epoch in range(num_epochs):\n",
    "    resnet50.train()\n",
    "    for i, (X, Y, W) in enumerate(train_dataset):\n",
    "        # Forward pass\n",
    "        X, Y, W = X.to(device), Y.to(device), W.to(device)\n",
    "\n",
    "        X = transformToTensor(X.cpu().data)\n",
    "\n",
    "        output = resnet50(X)\n",
    "        criterion.weight = all_weights\n",
    "        loss = criterion(output, Y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    y_pred_list = np.empty(0)\n",
    "    y_target_list = np.empty(0)\n",
    "    y_weight_list = np.empty(0)\n",
    "    resnet50.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y, W) in enumerate(valid_dataset):\n",
    "            X, Y, W = X.to(device), Y.to(device), W.to(device)\n",
    "\n",
    "            X = transformToTensor(X.cpu().data)\n",
    "\n",
    "            output = resnet50(X)\n",
    "\n",
    "            y_pred_list = np.concatenate((y_pred_list, (np.argmax(np.array(output.cpu().data), axis = 1))), axis = 0)\n",
    "\n",
    "            y_target_list = np.concatenate((y_target_list, (np.argmax(np.array(Y.cpu().data), axis=1))), axis= 0)\n",
    "\n",
    "            y_weight_list = np.concatenate((y_weight_list, np.array(W.cpu().data)), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    conf_list.append(confusion_matrix(y_pred_list,y_target_list))\n",
    "    precise_list.append(precision_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    recall_list.append(recall_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    f1_list.append(f1_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    weightacc_list.append(weight_accuracy(y_pred_list,y_target_list, y_weight_list))\n",
    "    numacc_list.append(num_accuracy(y_pred_list,y_target_list))\n",
    "\n",
    "    print(\"Accuracy:\", weight_accuracy(y_pred_list,y_target_list, y_weight_list))\n",
    "    print(\"Num Accuracy:\", num_accuracy(y_pred_list,y_target_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "f6706e41-7016-4d4e-9ae3-de762847f85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [1/63], Loss: 24.9255\n",
      "Epoch [1/1], Step [2/63], Loss: 82.8100\n",
      "Epoch [1/1], Step [3/63], Loss: 66.6724\n",
      "Epoch [1/1], Step [4/63], Loss: 125.3123\n",
      "Epoch [1/1], Step [5/63], Loss: 79.3465\n",
      "Epoch [1/1], Step [6/63], Loss: 68.4869\n",
      "Epoch [1/1], Step [7/63], Loss: 117.9308\n",
      "Epoch [1/1], Step [8/63], Loss: 80.8077\n",
      "Epoch [1/1], Step [9/63], Loss: 89.5297\n",
      "Epoch [1/1], Step [10/63], Loss: 50.9546\n",
      "Epoch [1/1], Step [11/63], Loss: 48.4022\n",
      "Epoch [1/1], Step [12/63], Loss: 51.8792\n",
      "Epoch [1/1], Step [13/63], Loss: 24.2481\n",
      "Epoch [1/1], Step [14/63], Loss: 19.6169\n",
      "Epoch [1/1], Step [15/63], Loss: 17.2331\n",
      "Epoch [1/1], Step [16/63], Loss: 13.1091\n",
      "Epoch [1/1], Step [17/63], Loss: 8.7620\n",
      "Epoch [1/1], Step [18/63], Loss: 11.1857\n",
      "Epoch [1/1], Step [19/63], Loss: 5.3491\n",
      "Epoch [1/1], Step [20/63], Loss: 4.4548\n",
      "Epoch [1/1], Step [21/63], Loss: 3.9092\n",
      "Epoch [1/1], Step [22/63], Loss: 2.7961\n",
      "Epoch [1/1], Step [23/63], Loss: 2.5295\n",
      "Epoch [1/1], Step [24/63], Loss: 3.3422\n",
      "Epoch [1/1], Step [25/63], Loss: 2.8122\n",
      "Epoch [1/1], Step [26/63], Loss: 3.0212\n",
      "Epoch [1/1], Step [27/63], Loss: 2.0834\n",
      "Epoch [1/1], Step [28/63], Loss: 2.6045\n",
      "Epoch [1/1], Step [29/63], Loss: 1.5966\n",
      "Epoch [1/1], Step [30/63], Loss: 1.2291\n",
      "Epoch [1/1], Step [31/63], Loss: 2.0103\n",
      "Epoch [1/1], Step [32/63], Loss: 1.4027\n",
      "Epoch [1/1], Step [33/63], Loss: 2.0404\n",
      "Epoch [1/1], Step [34/63], Loss: 1.2416\n",
      "Epoch [1/1], Step [35/63], Loss: 1.4476\n",
      "Epoch [1/1], Step [36/63], Loss: 1.2697\n",
      "Epoch [1/1], Step [37/63], Loss: 1.1927\n",
      "Epoch [1/1], Step [38/63], Loss: 1.1890\n",
      "Epoch [1/1], Step [39/63], Loss: 0.9450\n",
      "Epoch [1/1], Step [40/63], Loss: 0.9942\n",
      "Epoch [1/1], Step [41/63], Loss: 1.1891\n",
      "Epoch [1/1], Step [42/63], Loss: 1.0581\n",
      "Epoch [1/1], Step [43/63], Loss: 1.0786\n",
      "Epoch [1/1], Step [44/63], Loss: 1.2083\n",
      "Epoch [1/1], Step [45/63], Loss: 0.7322\n",
      "Epoch [1/1], Step [46/63], Loss: 1.0364\n",
      "Epoch [1/1], Step [47/63], Loss: 1.0088\n",
      "Epoch [1/1], Step [48/63], Loss: 0.6307\n",
      "Epoch [1/1], Step [49/63], Loss: 0.6441\n",
      "Epoch [1/1], Step [50/63], Loss: 0.6673\n",
      "Epoch [1/1], Step [51/63], Loss: 0.6939\n",
      "Epoch [1/1], Step [52/63], Loss: 0.5228\n",
      "Epoch [1/1], Step [53/63], Loss: 1.0280\n",
      "Epoch [1/1], Step [54/63], Loss: 0.7711\n",
      "Epoch [1/1], Step [55/63], Loss: 0.5913\n",
      "Epoch [1/1], Step [56/63], Loss: 0.9780\n",
      "Epoch [1/1], Step [57/63], Loss: 0.8249\n",
      "Epoch [1/1], Step [58/63], Loss: 0.7815\n",
      "Epoch [1/1], Step [59/63], Loss: 0.6786\n",
      "Epoch [1/1], Step [60/63], Loss: 0.7816\n",
      "Epoch [1/1], Step [61/63], Loss: 0.5178\n",
      "Epoch [1/1], Step [62/63], Loss: 0.4616\n",
      "Epoch [1/1], Step [63/63], Loss: 0.4479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Z:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.15293166642815922\n",
      "Num Accuracy: 0.6280579131303046\n",
      "Epoch [2/1], Step [1/63], Loss: 0.5931\n",
      "Epoch [2/1], Step [2/63], Loss: 0.7842\n",
      "Epoch [2/1], Step [3/63], Loss: 0.5912\n",
      "Epoch [2/1], Step [4/63], Loss: 0.5022\n",
      "Epoch [2/1], Step [5/63], Loss: 0.4862\n",
      "Epoch [2/1], Step [6/63], Loss: 0.6970\n",
      "Epoch [2/1], Step [7/63], Loss: 0.5329\n",
      "Epoch [2/1], Step [8/63], Loss: 0.5751\n",
      "Epoch [2/1], Step [9/63], Loss: 0.5537\n",
      "Epoch [2/1], Step [10/63], Loss: 0.4612\n",
      "Epoch [2/1], Step [11/63], Loss: 0.4783\n",
      "Epoch [2/1], Step [12/63], Loss: 0.4080\n",
      "Epoch [2/1], Step [13/63], Loss: 0.6936\n",
      "Epoch [2/1], Step [14/63], Loss: 0.5996\n",
      "Epoch [2/1], Step [15/63], Loss: 0.5001\n",
      "Epoch [2/1], Step [16/63], Loss: 0.4534\n",
      "Epoch [2/1], Step [17/63], Loss: 0.5137\n",
      "Epoch [2/1], Step [18/63], Loss: 0.5149\n",
      "Epoch [2/1], Step [19/63], Loss: 0.3783\n",
      "Epoch [2/1], Step [20/63], Loss: 0.4799\n",
      "Epoch [2/1], Step [21/63], Loss: 0.5594\n",
      "Epoch [2/1], Step [22/63], Loss: 0.6630\n",
      "Epoch [2/1], Step [23/63], Loss: 0.5351\n",
      "Epoch [2/1], Step [24/63], Loss: 0.5606\n",
      "Epoch [2/1], Step [25/63], Loss: 0.3619\n",
      "Epoch [2/1], Step [26/63], Loss: 0.4216\n",
      "Epoch [2/1], Step [27/63], Loss: 0.5826\n",
      "Epoch [2/1], Step [28/63], Loss: 0.4802\n",
      "Epoch [2/1], Step [29/63], Loss: 0.8149\n",
      "Epoch [2/1], Step [30/63], Loss: 0.4334\n",
      "Epoch [2/1], Step [31/63], Loss: 0.4282\n",
      "Epoch [2/1], Step [32/63], Loss: 0.3727\n",
      "Epoch [2/1], Step [33/63], Loss: 0.4767\n",
      "Epoch [2/1], Step [34/63], Loss: 0.5691\n",
      "Epoch [2/1], Step [35/63], Loss: 0.6405\n",
      "Epoch [2/1], Step [36/63], Loss: 0.7419\n",
      "Epoch [2/1], Step [37/63], Loss: 0.5384\n",
      "Epoch [2/1], Step [38/63], Loss: 0.3203\n",
      "Epoch [2/1], Step [39/63], Loss: 0.4287\n",
      "Epoch [2/1], Step [40/63], Loss: 0.5141\n",
      "Epoch [2/1], Step [41/63], Loss: 0.4502\n",
      "Epoch [2/1], Step [42/63], Loss: 0.6116\n",
      "Epoch [2/1], Step [43/63], Loss: 0.4642\n",
      "Epoch [2/1], Step [44/63], Loss: 0.5120\n",
      "Epoch [2/1], Step [45/63], Loss: 0.3927\n",
      "Epoch [2/1], Step [46/63], Loss: 0.4034\n",
      "Epoch [2/1], Step [47/63], Loss: 0.4683\n",
      "Epoch [2/1], Step [48/63], Loss: 0.4517\n",
      "Epoch [2/1], Step [49/63], Loss: 0.3729\n",
      "Epoch [2/1], Step [50/63], Loss: 0.5362\n",
      "Epoch [2/1], Step [51/63], Loss: 0.3049\n",
      "Epoch [2/1], Step [52/63], Loss: 0.5926\n",
      "Epoch [2/1], Step [53/63], Loss: 0.4327\n",
      "Epoch [2/1], Step [54/63], Loss: 0.5369\n",
      "Epoch [2/1], Step [55/63], Loss: 0.5500\n",
      "Epoch [2/1], Step [56/63], Loss: 0.3900\n",
      "Epoch [2/1], Step [57/63], Loss: 0.5819\n",
      "Epoch [2/1], Step [58/63], Loss: 0.4826\n",
      "Epoch [2/1], Step [59/63], Loss: 0.3374\n",
      "Epoch [2/1], Step [60/63], Loss: 0.4255\n",
      "Epoch [2/1], Step [61/63], Loss: 0.3782\n",
      "Epoch [2/1], Step [62/63], Loss: 0.4206\n",
      "Epoch [2/1], Step [63/63], Loss: 0.5330\n",
      "Accuracy: 0.16243925136838364\n",
      "Num Accuracy: 0.6430354468297553\n",
      "Epoch [3/1], Step [1/63], Loss: 0.3601\n",
      "Epoch [3/1], Step [2/63], Loss: 0.3888\n",
      "Epoch [3/1], Step [3/63], Loss: 0.3356\n",
      "Epoch [3/1], Step [4/63], Loss: 0.4679\n",
      "Epoch [3/1], Step [5/63], Loss: 0.4028\n",
      "Epoch [3/1], Step [6/63], Loss: 0.3362\n",
      "Epoch [3/1], Step [7/63], Loss: 0.5501\n",
      "Epoch [3/1], Step [8/63], Loss: 0.4806\n",
      "Epoch [3/1], Step [9/63], Loss: 0.3036\n",
      "Epoch [3/1], Step [10/63], Loss: 0.4536\n",
      "Epoch [3/1], Step [11/63], Loss: 0.4362\n",
      "Epoch [3/1], Step [12/63], Loss: 0.3326\n",
      "Epoch [3/1], Step [13/63], Loss: 0.5507\n",
      "Epoch [3/1], Step [14/63], Loss: 0.4272\n",
      "Epoch [3/1], Step [15/63], Loss: 0.3744\n",
      "Epoch [3/1], Step [16/63], Loss: 0.3342\n",
      "Epoch [3/1], Step [17/63], Loss: 0.3257\n",
      "Epoch [3/1], Step [18/63], Loss: 0.4652\n",
      "Epoch [3/1], Step [19/63], Loss: 0.4100\n",
      "Epoch [3/1], Step [20/63], Loss: 0.3961\n",
      "Epoch [3/1], Step [21/63], Loss: 0.5049\n",
      "Epoch [3/1], Step [22/63], Loss: 0.4332\n",
      "Epoch [3/1], Step [23/63], Loss: 0.4208\n",
      "Epoch [3/1], Step [24/63], Loss: 0.4124\n",
      "Epoch [3/1], Step [25/63], Loss: 0.4558\n",
      "Epoch [3/1], Step [26/63], Loss: 0.3077\n",
      "Epoch [3/1], Step [27/63], Loss: 0.4405\n",
      "Epoch [3/1], Step [28/63], Loss: 0.3131\n",
      "Epoch [3/1], Step [29/63], Loss: 0.4811\n",
      "Epoch [3/1], Step [30/63], Loss: 0.3745\n",
      "Epoch [3/1], Step [31/63], Loss: 0.4163\n",
      "Epoch [3/1], Step [32/63], Loss: 0.4019\n",
      "Epoch [3/1], Step [33/63], Loss: 0.4263\n",
      "Epoch [3/1], Step [34/63], Loss: 0.6433\n",
      "Epoch [3/1], Step [35/63], Loss: 0.2549\n",
      "Epoch [3/1], Step [36/63], Loss: 0.3390\n",
      "Epoch [3/1], Step [37/63], Loss: 0.3189\n",
      "Epoch [3/1], Step [38/63], Loss: 0.4467\n",
      "Epoch [3/1], Step [39/63], Loss: 0.3822\n",
      "Epoch [3/1], Step [40/63], Loss: 0.3630\n",
      "Epoch [3/1], Step [41/63], Loss: 0.2793\n",
      "Epoch [3/1], Step [42/63], Loss: 0.4777\n",
      "Epoch [3/1], Step [43/63], Loss: 0.4082\n",
      "Epoch [3/1], Step [44/63], Loss: 0.3170\n",
      "Epoch [3/1], Step [45/63], Loss: 0.4195\n",
      "Epoch [3/1], Step [46/63], Loss: 0.3307\n",
      "Epoch [3/1], Step [47/63], Loss: 0.4573\n",
      "Epoch [3/1], Step [48/63], Loss: 0.4732\n",
      "Epoch [3/1], Step [49/63], Loss: 0.4805\n",
      "Epoch [3/1], Step [50/63], Loss: 0.5058\n",
      "Epoch [3/1], Step [51/63], Loss: 0.4396\n",
      "Epoch [3/1], Step [52/63], Loss: 0.4889\n",
      "Epoch [3/1], Step [53/63], Loss: 0.5037\n",
      "Epoch [3/1], Step [54/63], Loss: 0.4094\n",
      "Epoch [3/1], Step [55/63], Loss: 0.3387\n",
      "Epoch [3/1], Step [56/63], Loss: 0.5219\n",
      "Epoch [3/1], Step [57/63], Loss: 0.3582\n",
      "Epoch [3/1], Step [58/63], Loss: 0.4372\n",
      "Epoch [3/1], Step [59/63], Loss: 0.3026\n",
      "Epoch [3/1], Step [60/63], Loss: 0.3534\n",
      "Epoch [3/1], Step [61/63], Loss: 0.3225\n",
      "Epoch [3/1], Step [62/63], Loss: 0.3384\n",
      "Epoch [3/1], Step [63/63], Loss: 0.3993\n",
      "Accuracy: 0.1698400253176698\n",
      "Num Accuracy: 0.6305541687468796\n",
      "Epoch [4/1], Step [1/63], Loss: 0.3758\n",
      "Epoch [4/1], Step [2/63], Loss: 0.4348\n",
      "Epoch [4/1], Step [3/63], Loss: 0.3830\n",
      "Epoch [4/1], Step [4/63], Loss: 0.4112\n",
      "Epoch [4/1], Step [5/63], Loss: 0.2642\n",
      "Epoch [4/1], Step [6/63], Loss: 0.2422\n",
      "Epoch [4/1], Step [7/63], Loss: 0.3319\n",
      "Epoch [4/1], Step [8/63], Loss: 0.2662\n",
      "Epoch [4/1], Step [9/63], Loss: 0.3304\n",
      "Epoch [4/1], Step [10/63], Loss: 0.3259\n",
      "Epoch [4/1], Step [11/63], Loss: 0.3627\n",
      "Epoch [4/1], Step [12/63], Loss: 0.3240\n",
      "Epoch [4/1], Step [13/63], Loss: 0.4451\n",
      "Epoch [4/1], Step [14/63], Loss: 0.3867\n",
      "Epoch [4/1], Step [15/63], Loss: 0.2954\n",
      "Epoch [4/1], Step [16/63], Loss: 0.3705\n",
      "Epoch [4/1], Step [17/63], Loss: 0.4550\n",
      "Epoch [4/1], Step [18/63], Loss: 0.3988\n",
      "Epoch [4/1], Step [19/63], Loss: 0.3492\n",
      "Epoch [4/1], Step [20/63], Loss: 0.3101\n",
      "Epoch [4/1], Step [21/63], Loss: 0.3429\n",
      "Epoch [4/1], Step [22/63], Loss: 0.2708\n",
      "Epoch [4/1], Step [23/63], Loss: 0.3548\n",
      "Epoch [4/1], Step [24/63], Loss: 0.3736\n",
      "Epoch [4/1], Step [25/63], Loss: 0.3510\n",
      "Epoch [4/1], Step [26/63], Loss: 0.2894\n",
      "Epoch [4/1], Step [27/63], Loss: 0.3531\n",
      "Epoch [4/1], Step [28/63], Loss: 0.2627\n",
      "Epoch [4/1], Step [29/63], Loss: 0.3243\n",
      "Epoch [4/1], Step [30/63], Loss: 0.2870\n",
      "Epoch [4/1], Step [31/63], Loss: 0.2632\n",
      "Epoch [4/1], Step [32/63], Loss: 0.2256\n",
      "Epoch [4/1], Step [33/63], Loss: 0.3569\n",
      "Epoch [4/1], Step [34/63], Loss: 0.3870\n",
      "Epoch [4/1], Step [35/63], Loss: 0.2707\n",
      "Epoch [4/1], Step [36/63], Loss: 0.3001\n",
      "Epoch [4/1], Step [37/63], Loss: 0.2684\n",
      "Epoch [4/1], Step [38/63], Loss: 0.3462\n",
      "Epoch [4/1], Step [39/63], Loss: 0.3775\n",
      "Epoch [4/1], Step [40/63], Loss: 0.2870\n",
      "Epoch [4/1], Step [41/63], Loss: 0.4211\n",
      "Epoch [4/1], Step [42/63], Loss: 0.3324\n",
      "Epoch [4/1], Step [43/63], Loss: 0.3461\n",
      "Epoch [4/1], Step [44/63], Loss: 0.3310\n",
      "Epoch [4/1], Step [45/63], Loss: 0.3328\n",
      "Epoch [4/1], Step [46/63], Loss: 0.3493\n",
      "Epoch [4/1], Step [47/63], Loss: 0.3789\n",
      "Epoch [4/1], Step [48/63], Loss: 0.3341\n",
      "Epoch [4/1], Step [49/63], Loss: 0.3040\n",
      "Epoch [4/1], Step [50/63], Loss: 0.2876\n",
      "Epoch [4/1], Step [51/63], Loss: 0.2478\n",
      "Epoch [4/1], Step [52/63], Loss: 0.2522\n",
      "Epoch [4/1], Step [53/63], Loss: 0.4227\n",
      "Epoch [4/1], Step [54/63], Loss: 0.3939\n",
      "Epoch [4/1], Step [55/63], Loss: 0.2822\n",
      "Epoch [4/1], Step [56/63], Loss: 0.3405\n",
      "Epoch [4/1], Step [57/63], Loss: 0.3728\n",
      "Epoch [4/1], Step [58/63], Loss: 0.3296\n",
      "Epoch [4/1], Step [59/63], Loss: 0.3644\n",
      "Epoch [4/1], Step [60/63], Loss: 0.2782\n",
      "Epoch [4/1], Step [61/63], Loss: 0.4233\n",
      "Epoch [4/1], Step [62/63], Loss: 0.3055\n",
      "Epoch [4/1], Step [63/63], Loss: 0.3686\n",
      "Accuracy: 0.16335118196713017\n",
      "Num Accuracy: 0.5476784822765851\n",
      "Epoch [5/1], Step [1/63], Loss: 0.2800\n",
      "Epoch [5/1], Step [2/63], Loss: 0.2469\n",
      "Epoch [5/1], Step [3/63], Loss: 0.3174\n",
      "Epoch [5/1], Step [4/63], Loss: 0.2099\n",
      "Epoch [5/1], Step [5/63], Loss: 0.2666\n",
      "Epoch [5/1], Step [6/63], Loss: 0.2269\n",
      "Epoch [5/1], Step [7/63], Loss: 0.2686\n",
      "Epoch [5/1], Step [8/63], Loss: 0.2615\n",
      "Epoch [5/1], Step [9/63], Loss: 0.2116\n",
      "Epoch [5/1], Step [10/63], Loss: 0.1897\n",
      "Epoch [5/1], Step [11/63], Loss: 0.3054\n",
      "Epoch [5/1], Step [12/63], Loss: 0.2497\n",
      "Epoch [5/1], Step [13/63], Loss: 0.2190\n",
      "Epoch [5/1], Step [14/63], Loss: 0.2330\n",
      "Epoch [5/1], Step [15/63], Loss: 0.2309\n",
      "Epoch [5/1], Step [16/63], Loss: 0.2161\n",
      "Epoch [5/1], Step [17/63], Loss: 0.2082\n",
      "Epoch [5/1], Step [18/63], Loss: 0.2596\n",
      "Epoch [5/1], Step [19/63], Loss: 0.4012\n",
      "Epoch [5/1], Step [20/63], Loss: 0.3205\n",
      "Epoch [5/1], Step [21/63], Loss: 0.2321\n",
      "Epoch [5/1], Step [22/63], Loss: 0.2596\n",
      "Epoch [5/1], Step [23/63], Loss: 0.1747\n",
      "Epoch [5/1], Step [24/63], Loss: 0.1977\n",
      "Epoch [5/1], Step [25/63], Loss: 0.2677\n",
      "Epoch [5/1], Step [26/63], Loss: 0.2539\n",
      "Epoch [5/1], Step [27/63], Loss: 0.2608\n",
      "Epoch [5/1], Step [28/63], Loss: 0.2566\n",
      "Epoch [5/1], Step [29/63], Loss: 0.3072\n",
      "Epoch [5/1], Step [30/63], Loss: 0.2230\n",
      "Epoch [5/1], Step [31/63], Loss: 0.3701\n",
      "Epoch [5/1], Step [32/63], Loss: 0.3083\n",
      "Epoch [5/1], Step [33/63], Loss: 0.3004\n",
      "Epoch [5/1], Step [34/63], Loss: 0.2925\n",
      "Epoch [5/1], Step [35/63], Loss: 0.2605\n",
      "Epoch [5/1], Step [36/63], Loss: 0.2444\n",
      "Epoch [5/1], Step [37/63], Loss: 0.2107\n",
      "Epoch [5/1], Step [38/63], Loss: 0.2487\n",
      "Epoch [5/1], Step [39/63], Loss: 0.2997\n",
      "Epoch [5/1], Step [40/63], Loss: 0.2805\n",
      "Epoch [5/1], Step [41/63], Loss: 0.3180\n",
      "Epoch [5/1], Step [42/63], Loss: 0.3027\n",
      "Epoch [5/1], Step [43/63], Loss: 0.2856\n",
      "Epoch [5/1], Step [44/63], Loss: 0.3021\n",
      "Epoch [5/1], Step [45/63], Loss: 0.3540\n",
      "Epoch [5/1], Step [46/63], Loss: 0.2660\n",
      "Epoch [5/1], Step [47/63], Loss: 0.2438\n",
      "Epoch [5/1], Step [48/63], Loss: 0.2313\n",
      "Epoch [5/1], Step [49/63], Loss: 0.2840\n",
      "Epoch [5/1], Step [50/63], Loss: 0.3536\n",
      "Epoch [5/1], Step [51/63], Loss: 0.2023\n",
      "Epoch [5/1], Step [52/63], Loss: 0.2586\n",
      "Epoch [5/1], Step [53/63], Loss: 0.2088\n",
      "Epoch [5/1], Step [54/63], Loss: 0.2840\n",
      "Epoch [5/1], Step [55/63], Loss: 0.3179\n",
      "Epoch [5/1], Step [56/63], Loss: 0.2820\n",
      "Epoch [5/1], Step [57/63], Loss: 0.2506\n",
      "Epoch [5/1], Step [58/63], Loss: 0.3282\n",
      "Epoch [5/1], Step [59/63], Loss: 0.2778\n",
      "Epoch [5/1], Step [60/63], Loss: 0.2503\n",
      "Epoch [5/1], Step [61/63], Loss: 0.2497\n",
      "Epoch [5/1], Step [62/63], Loss: 0.2058\n",
      "Epoch [5/1], Step [63/63], Loss: 0.2852\n",
      "Accuracy: 0.2315193513623446\n",
      "Num Accuracy: 0.5711432850723914\n",
      "Epoch [6/1], Step [1/63], Loss: 0.2698\n",
      "Epoch [6/1], Step [2/63], Loss: 0.1633\n",
      "Epoch [6/1], Step [3/63], Loss: 0.1716\n",
      "Epoch [6/1], Step [4/63], Loss: 0.2042\n",
      "Epoch [6/1], Step [5/63], Loss: 0.2564\n",
      "Epoch [6/1], Step [6/63], Loss: 0.1744\n",
      "Epoch [6/1], Step [7/63], Loss: 0.1354\n",
      "Epoch [6/1], Step [8/63], Loss: 0.2206\n",
      "Epoch [6/1], Step [9/63], Loss: 0.1962\n",
      "Epoch [6/1], Step [10/63], Loss: 0.1606\n",
      "Epoch [6/1], Step [11/63], Loss: 0.2203\n",
      "Epoch [6/1], Step [12/63], Loss: 0.1857\n",
      "Epoch [6/1], Step [13/63], Loss: 0.1589\n",
      "Epoch [6/1], Step [14/63], Loss: 0.1694\n",
      "Epoch [6/1], Step [15/63], Loss: 0.2680\n",
      "Epoch [6/1], Step [16/63], Loss: 0.1934\n",
      "Epoch [6/1], Step [17/63], Loss: 0.2687\n",
      "Epoch [6/1], Step [18/63], Loss: 0.2205\n",
      "Epoch [6/1], Step [19/63], Loss: 0.2111\n",
      "Epoch [6/1], Step [20/63], Loss: 0.2283\n",
      "Epoch [6/1], Step [21/63], Loss: 0.1509\n",
      "Epoch [6/1], Step [22/63], Loss: 0.2274\n",
      "Epoch [6/1], Step [23/63], Loss: 0.2124\n",
      "Epoch [6/1], Step [24/63], Loss: 0.1728\n",
      "Epoch [6/1], Step [25/63], Loss: 0.2840\n",
      "Epoch [6/1], Step [26/63], Loss: 0.2385\n",
      "Epoch [6/1], Step [27/63], Loss: 0.3215\n",
      "Epoch [6/1], Step [28/63], Loss: 0.2565\n",
      "Epoch [6/1], Step [29/63], Loss: 0.1968\n",
      "Epoch [6/1], Step [30/63], Loss: 0.2162\n",
      "Epoch [6/1], Step [31/63], Loss: 0.2263\n",
      "Epoch [6/1], Step [32/63], Loss: 0.2290\n",
      "Epoch [6/1], Step [33/63], Loss: 0.3114\n",
      "Epoch [6/1], Step [34/63], Loss: 0.1976\n",
      "Epoch [6/1], Step [35/63], Loss: 0.2615\n",
      "Epoch [6/1], Step [36/63], Loss: 0.2872\n",
      "Epoch [6/1], Step [37/63], Loss: 0.2226\n",
      "Epoch [6/1], Step [38/63], Loss: 0.2035\n",
      "Epoch [6/1], Step [39/63], Loss: 0.2520\n",
      "Epoch [6/1], Step [40/63], Loss: 0.2266\n",
      "Epoch [6/1], Step [41/63], Loss: 0.1999\n",
      "Epoch [6/1], Step [42/63], Loss: 0.2492\n",
      "Epoch [6/1], Step [43/63], Loss: 0.1894\n",
      "Epoch [6/1], Step [44/63], Loss: 0.2281\n",
      "Epoch [6/1], Step [45/63], Loss: 0.2366\n",
      "Epoch [6/1], Step [46/63], Loss: 0.2219\n",
      "Epoch [6/1], Step [47/63], Loss: 0.2872\n",
      "Epoch [6/1], Step [48/63], Loss: 0.2635\n",
      "Epoch [6/1], Step [49/63], Loss: 0.3045\n",
      "Epoch [6/1], Step [50/63], Loss: 0.1922\n",
      "Epoch [6/1], Step [51/63], Loss: 0.2210\n",
      "Epoch [6/1], Step [52/63], Loss: 0.1816\n",
      "Epoch [6/1], Step [53/63], Loss: 0.2205\n",
      "Epoch [6/1], Step [54/63], Loss: 0.2511\n",
      "Epoch [6/1], Step [55/63], Loss: 0.2425\n",
      "Epoch [6/1], Step [56/63], Loss: 0.1850\n",
      "Epoch [6/1], Step [57/63], Loss: 0.2261\n",
      "Epoch [6/1], Step [58/63], Loss: 0.2155\n",
      "Epoch [6/1], Step [59/63], Loss: 0.2479\n",
      "Epoch [6/1], Step [60/63], Loss: 0.3095\n",
      "Epoch [6/1], Step [61/63], Loss: 0.2151\n",
      "Epoch [6/1], Step [62/63], Loss: 0.2871\n",
      "Epoch [6/1], Step [63/63], Loss: 0.3220\n",
      "Accuracy: 0.19896933303351763\n",
      "Num Accuracy: 0.5157264103844234\n",
      "Epoch [7/1], Step [1/63], Loss: 0.1703\n",
      "Epoch [7/1], Step [2/63], Loss: 0.1772\n",
      "Epoch [7/1], Step [3/63], Loss: 0.1622\n",
      "Epoch [7/1], Step [4/63], Loss: 0.2184\n",
      "Epoch [7/1], Step [5/63], Loss: 0.1634\n",
      "Epoch [7/1], Step [6/63], Loss: 0.1893\n",
      "Epoch [7/1], Step [7/63], Loss: 0.1207\n",
      "Epoch [7/1], Step [8/63], Loss: 0.1582\n",
      "Epoch [7/1], Step [9/63], Loss: 0.1810\n",
      "Epoch [7/1], Step [10/63], Loss: 0.1309\n",
      "Epoch [7/1], Step [11/63], Loss: 0.1662\n",
      "Epoch [7/1], Step [12/63], Loss: 0.1727\n",
      "Epoch [7/1], Step [13/63], Loss: 0.1638\n",
      "Epoch [7/1], Step [14/63], Loss: 0.1487\n",
      "Epoch [7/1], Step [15/63], Loss: 0.1345\n",
      "Epoch [7/1], Step [16/63], Loss: 0.1434\n",
      "Epoch [7/1], Step [17/63], Loss: 0.2584\n",
      "Epoch [7/1], Step [18/63], Loss: 0.1332\n",
      "Epoch [7/1], Step [19/63], Loss: 0.1156\n",
      "Epoch [7/1], Step [20/63], Loss: 0.1893\n",
      "Epoch [7/1], Step [21/63], Loss: 0.2063\n",
      "Epoch [7/1], Step [22/63], Loss: 0.1884\n",
      "Epoch [7/1], Step [23/63], Loss: 0.2380\n",
      "Epoch [7/1], Step [24/63], Loss: 0.1555\n",
      "Epoch [7/1], Step [25/63], Loss: 0.2258\n",
      "Epoch [7/1], Step [26/63], Loss: 0.1942\n",
      "Epoch [7/1], Step [27/63], Loss: 0.2154\n",
      "Epoch [7/1], Step [28/63], Loss: 0.2765\n",
      "Epoch [7/1], Step [29/63], Loss: 0.2284\n",
      "Epoch [7/1], Step [30/63], Loss: 0.1929\n",
      "Epoch [7/1], Step [31/63], Loss: 0.2238\n",
      "Epoch [7/1], Step [32/63], Loss: 0.1423\n",
      "Epoch [7/1], Step [33/63], Loss: 0.2267\n",
      "Epoch [7/1], Step [34/63], Loss: 0.2019\n",
      "Epoch [7/1], Step [35/63], Loss: 0.2993\n",
      "Epoch [7/1], Step [36/63], Loss: 0.2025\n",
      "Epoch [7/1], Step [37/63], Loss: 0.1926\n",
      "Epoch [7/1], Step [38/63], Loss: 0.2104\n",
      "Epoch [7/1], Step [39/63], Loss: 0.2005\n",
      "Epoch [7/1], Step [40/63], Loss: 0.1593\n",
      "Epoch [7/1], Step [41/63], Loss: 0.1804\n",
      "Epoch [7/1], Step [42/63], Loss: 0.0968\n",
      "Epoch [7/1], Step [43/63], Loss: 0.1284\n",
      "Epoch [7/1], Step [44/63], Loss: 0.3025\n",
      "Epoch [7/1], Step [45/63], Loss: 0.2319\n",
      "Epoch [7/1], Step [46/63], Loss: 0.2530\n",
      "Epoch [7/1], Step [47/63], Loss: 0.2372\n",
      "Epoch [7/1], Step [48/63], Loss: 0.1571\n",
      "Epoch [7/1], Step [49/63], Loss: 0.1647\n",
      "Epoch [7/1], Step [50/63], Loss: 0.2581\n",
      "Epoch [7/1], Step [51/63], Loss: 0.2403\n",
      "Epoch [7/1], Step [52/63], Loss: 0.1671\n",
      "Epoch [7/1], Step [53/63], Loss: 0.1706\n",
      "Epoch [7/1], Step [54/63], Loss: 0.2892\n",
      "Epoch [7/1], Step [55/63], Loss: 0.2090\n",
      "Epoch [7/1], Step [56/63], Loss: 0.2742\n",
      "Epoch [7/1], Step [57/63], Loss: 0.1640\n",
      "Epoch [7/1], Step [58/63], Loss: 0.2096\n",
      "Epoch [7/1], Step [59/63], Loss: 0.1598\n",
      "Epoch [7/1], Step [60/63], Loss: 0.2118\n",
      "Epoch [7/1], Step [61/63], Loss: 0.2151\n",
      "Epoch [7/1], Step [62/63], Loss: 0.1699\n",
      "Epoch [7/1], Step [63/63], Loss: 0.1755\n",
      "Accuracy: 0.2007890925640125\n",
      "Num Accuracy: 0.5082376435346979\n",
      "Epoch [8/1], Step [1/63], Loss: 0.0958\n",
      "Epoch [8/1], Step [2/63], Loss: 0.1641\n",
      "Epoch [8/1], Step [3/63], Loss: 0.1483\n",
      "Epoch [8/1], Step [4/63], Loss: 0.1545\n",
      "Epoch [8/1], Step [5/63], Loss: 0.1445\n",
      "Epoch [8/1], Step [6/63], Loss: 0.1411\n",
      "Epoch [8/1], Step [7/63], Loss: 0.1343\n",
      "Epoch [8/1], Step [8/63], Loss: 0.1634\n",
      "Epoch [8/1], Step [9/63], Loss: 0.1870\n",
      "Epoch [8/1], Step [10/63], Loss: 0.1057\n",
      "Epoch [8/1], Step [11/63], Loss: 0.1192\n",
      "Epoch [8/1], Step [12/63], Loss: 0.1263\n",
      "Epoch [8/1], Step [13/63], Loss: 0.1406\n",
      "Epoch [8/1], Step [14/63], Loss: 0.1330\n",
      "Epoch [8/1], Step [15/63], Loss: 0.1278\n",
      "Epoch [8/1], Step [16/63], Loss: 0.1149\n",
      "Epoch [8/1], Step [17/63], Loss: 0.1342\n",
      "Epoch [8/1], Step [18/63], Loss: 0.0768\n",
      "Epoch [8/1], Step [19/63], Loss: 0.1029\n",
      "Epoch [8/1], Step [20/63], Loss: 0.1509\n",
      "Epoch [8/1], Step [21/63], Loss: 0.1261\n",
      "Epoch [8/1], Step [22/63], Loss: 0.1919\n",
      "Epoch [8/1], Step [23/63], Loss: 0.1201\n",
      "Epoch [8/1], Step [24/63], Loss: 0.1459\n",
      "Epoch [8/1], Step [25/63], Loss: 0.1257\n",
      "Epoch [8/1], Step [26/63], Loss: 0.1922\n",
      "Epoch [8/1], Step [27/63], Loss: 0.1738\n",
      "Epoch [8/1], Step [28/63], Loss: 0.1671\n",
      "Epoch [8/1], Step [29/63], Loss: 0.0954\n",
      "Epoch [8/1], Step [30/63], Loss: 0.1738\n",
      "Epoch [8/1], Step [31/63], Loss: 0.2119\n",
      "Epoch [8/1], Step [32/63], Loss: 0.1689\n",
      "Epoch [8/1], Step [33/63], Loss: 0.1310\n",
      "Epoch [8/1], Step [34/63], Loss: 0.1427\n",
      "Epoch [8/1], Step [35/63], Loss: 0.1579\n",
      "Epoch [8/1], Step [36/63], Loss: 0.1065\n",
      "Epoch [8/1], Step [37/63], Loss: 0.1583\n",
      "Epoch [8/1], Step [38/63], Loss: 0.1363\n",
      "Epoch [8/1], Step [39/63], Loss: 0.2426\n",
      "Epoch [8/1], Step [40/63], Loss: 0.1889\n",
      "Epoch [8/1], Step [41/63], Loss: 0.1095\n",
      "Epoch [8/1], Step [42/63], Loss: 0.1355\n",
      "Epoch [8/1], Step [43/63], Loss: 0.1262\n",
      "Epoch [8/1], Step [44/63], Loss: 0.1011\n",
      "Epoch [8/1], Step [45/63], Loss: 0.1215\n",
      "Epoch [8/1], Step [46/63], Loss: 0.2314\n",
      "Epoch [8/1], Step [47/63], Loss: 0.1954\n",
      "Epoch [8/1], Step [48/63], Loss: 0.1801\n",
      "Epoch [8/1], Step [49/63], Loss: 0.1117\n",
      "Epoch [8/1], Step [50/63], Loss: 0.2021\n",
      "Epoch [8/1], Step [51/63], Loss: 0.1524\n",
      "Epoch [8/1], Step [52/63], Loss: 0.2339\n",
      "Epoch [8/1], Step [53/63], Loss: 0.0941\n",
      "Epoch [8/1], Step [54/63], Loss: 0.1880\n",
      "Epoch [8/1], Step [55/63], Loss: 0.1600\n",
      "Epoch [8/1], Step [56/63], Loss: 0.2418\n",
      "Epoch [8/1], Step [57/63], Loss: 0.1722\n",
      "Epoch [8/1], Step [58/63], Loss: 0.2139\n",
      "Epoch [8/1], Step [59/63], Loss: 0.2324\n",
      "Epoch [8/1], Step [60/63], Loss: 0.1307\n",
      "Epoch [8/1], Step [61/63], Loss: 0.1896\n",
      "Epoch [8/1], Step [62/63], Loss: 0.1521\n",
      "Epoch [8/1], Step [63/63], Loss: 0.2294\n",
      "Accuracy: 0.21072349906942034\n",
      "Num Accuracy: 0.5312031952071892\n",
      "Epoch [9/1], Step [1/63], Loss: 0.1353\n",
      "Epoch [9/1], Step [2/63], Loss: 0.1082\n",
      "Epoch [9/1], Step [3/63], Loss: 0.1225\n",
      "Epoch [9/1], Step [4/63], Loss: 0.1296\n",
      "Epoch [9/1], Step [5/63], Loss: 0.1029\n",
      "Epoch [9/1], Step [6/63], Loss: 0.1283\n",
      "Epoch [9/1], Step [7/63], Loss: 0.1493\n",
      "Epoch [9/1], Step [8/63], Loss: 0.0797\n",
      "Epoch [9/1], Step [9/63], Loss: 0.1123\n",
      "Epoch [9/1], Step [10/63], Loss: 0.1139\n",
      "Epoch [9/1], Step [11/63], Loss: 0.1490\n",
      "Epoch [9/1], Step [12/63], Loss: 0.1135\n",
      "Epoch [9/1], Step [13/63], Loss: 0.0885\n",
      "Epoch [9/1], Step [14/63], Loss: 0.0995\n",
      "Epoch [9/1], Step [15/63], Loss: 0.1831\n",
      "Epoch [9/1], Step [16/63], Loss: 0.1265\n",
      "Epoch [9/1], Step [17/63], Loss: 0.1040\n",
      "Epoch [9/1], Step [18/63], Loss: 0.1604\n",
      "Epoch [9/1], Step [19/63], Loss: 0.1385\n",
      "Epoch [9/1], Step [20/63], Loss: 0.1286\n",
      "Epoch [9/1], Step [21/63], Loss: 0.1056\n",
      "Epoch [9/1], Step [22/63], Loss: 0.1192\n",
      "Epoch [9/1], Step [23/63], Loss: 0.1257\n",
      "Epoch [9/1], Step [24/63], Loss: 0.2001\n",
      "Epoch [9/1], Step [25/63], Loss: 0.0807\n",
      "Epoch [9/1], Step [26/63], Loss: 0.1150\n",
      "Epoch [9/1], Step [27/63], Loss: 0.1042\n",
      "Epoch [9/1], Step [28/63], Loss: 0.1396\n",
      "Epoch [9/1], Step [29/63], Loss: 0.1328\n",
      "Epoch [9/1], Step [30/63], Loss: 0.1197\n",
      "Epoch [9/1], Step [31/63], Loss: 0.1153\n",
      "Epoch [9/1], Step [32/63], Loss: 0.0693\n",
      "Epoch [9/1], Step [33/63], Loss: 0.0921\n",
      "Epoch [9/1], Step [34/63], Loss: 0.1231\n",
      "Epoch [9/1], Step [35/63], Loss: 0.1196\n",
      "Epoch [9/1], Step [36/63], Loss: 0.1331\n",
      "Epoch [9/1], Step [37/63], Loss: 0.0963\n",
      "Epoch [9/1], Step [38/63], Loss: 0.1792\n",
      "Epoch [9/1], Step [39/63], Loss: 0.1537\n",
      "Epoch [9/1], Step [40/63], Loss: 0.1391\n",
      "Epoch [9/1], Step [41/63], Loss: 0.1731\n",
      "Epoch [9/1], Step [42/63], Loss: 0.1382\n",
      "Epoch [9/1], Step [43/63], Loss: 0.1345\n",
      "Epoch [9/1], Step [44/63], Loss: 0.1674\n",
      "Epoch [9/1], Step [45/63], Loss: 0.0794\n",
      "Epoch [9/1], Step [46/63], Loss: 0.1577\n",
      "Epoch [9/1], Step [47/63], Loss: 0.1542\n",
      "Epoch [9/1], Step [48/63], Loss: 0.1828\n",
      "Epoch [9/1], Step [49/63], Loss: 0.1267\n",
      "Epoch [9/1], Step [50/63], Loss: 0.1812\n",
      "Epoch [9/1], Step [51/63], Loss: 0.1359\n",
      "Epoch [9/1], Step [52/63], Loss: 0.2197\n",
      "Epoch [9/1], Step [53/63], Loss: 0.1811\n",
      "Epoch [9/1], Step [54/63], Loss: 0.1956\n",
      "Epoch [9/1], Step [55/63], Loss: 0.1180\n",
      "Epoch [9/1], Step [56/63], Loss: 0.1647\n",
      "Epoch [9/1], Step [57/63], Loss: 0.2168\n",
      "Epoch [9/1], Step [58/63], Loss: 0.1447\n",
      "Epoch [9/1], Step [59/63], Loss: 0.1647\n",
      "Epoch [9/1], Step [60/63], Loss: 0.1375\n",
      "Epoch [9/1], Step [61/63], Loss: 0.1570\n",
      "Epoch [9/1], Step [62/63], Loss: 0.1952\n",
      "Epoch [9/1], Step [63/63], Loss: 0.1389\n",
      "Accuracy: 0.19611789710161068\n",
      "Num Accuracy: 0.4423364952571143\n",
      "Epoch [10/1], Step [1/63], Loss: 0.1699\n",
      "Epoch [10/1], Step [2/63], Loss: 0.0937\n",
      "Epoch [10/1], Step [3/63], Loss: 0.0970\n",
      "Epoch [10/1], Step [4/63], Loss: 0.1135\n",
      "Epoch [10/1], Step [5/63], Loss: 0.1062\n",
      "Epoch [10/1], Step [6/63], Loss: 0.1453\n",
      "Epoch [10/1], Step [7/63], Loss: 0.1066\n",
      "Epoch [10/1], Step [8/63], Loss: 0.1202\n",
      "Epoch [10/1], Step [9/63], Loss: 0.0833\n",
      "Epoch [10/1], Step [10/63], Loss: 0.1442\n",
      "Epoch [10/1], Step [11/63], Loss: 0.0966\n",
      "Epoch [10/1], Step [12/63], Loss: 0.0965\n",
      "Epoch [10/1], Step [13/63], Loss: 0.0794\n",
      "Epoch [10/1], Step [14/63], Loss: 0.0800\n",
      "Epoch [10/1], Step [15/63], Loss: 0.0920\n",
      "Epoch [10/1], Step [16/63], Loss: 0.1435\n",
      "Epoch [10/1], Step [17/63], Loss: 0.0574\n",
      "Epoch [10/1], Step [18/63], Loss: 0.1037\n",
      "Epoch [10/1], Step [19/63], Loss: 0.0939\n",
      "Epoch [10/1], Step [20/63], Loss: 0.0947\n",
      "Epoch [10/1], Step [21/63], Loss: 0.0733\n",
      "Epoch [10/1], Step [22/63], Loss: 0.1296\n",
      "Epoch [10/1], Step [23/63], Loss: 0.2038\n",
      "Epoch [10/1], Step [24/63], Loss: 0.1020\n",
      "Epoch [10/1], Step [25/63], Loss: 0.1211\n",
      "Epoch [10/1], Step [26/63], Loss: 0.1127\n",
      "Epoch [10/1], Step [27/63], Loss: 0.0909\n",
      "Epoch [10/1], Step [28/63], Loss: 0.1053\n",
      "Epoch [10/1], Step [29/63], Loss: 0.1139\n",
      "Epoch [10/1], Step [30/63], Loss: 0.1145\n",
      "Epoch [10/1], Step [31/63], Loss: 0.1621\n",
      "Epoch [10/1], Step [32/63], Loss: 0.1265\n",
      "Epoch [10/1], Step [33/63], Loss: 0.0734\n",
      "Epoch [10/1], Step [34/63], Loss: 0.1300\n",
      "Epoch [10/1], Step [35/63], Loss: 0.0760\n",
      "Epoch [10/1], Step [36/63], Loss: 0.1088\n",
      "Epoch [10/1], Step [37/63], Loss: 0.0825\n",
      "Epoch [10/1], Step [38/63], Loss: 0.1013\n",
      "Epoch [10/1], Step [39/63], Loss: 0.1362\n",
      "Epoch [10/1], Step [40/63], Loss: 0.1773\n",
      "Epoch [10/1], Step [41/63], Loss: 0.1369\n",
      "Epoch [10/1], Step [42/63], Loss: 0.0921\n",
      "Epoch [10/1], Step [43/63], Loss: 0.0785\n",
      "Epoch [10/1], Step [44/63], Loss: 0.1155\n",
      "Epoch [10/1], Step [45/63], Loss: 0.0831\n",
      "Epoch [10/1], Step [46/63], Loss: 0.0975\n",
      "Epoch [10/1], Step [47/63], Loss: 0.1383\n",
      "Epoch [10/1], Step [48/63], Loss: 0.1620\n",
      "Epoch [10/1], Step [49/63], Loss: 0.1402\n",
      "Epoch [10/1], Step [50/63], Loss: 0.1152\n",
      "Epoch [10/1], Step [51/63], Loss: 0.1791\n",
      "Epoch [10/1], Step [52/63], Loss: 0.1218\n",
      "Epoch [10/1], Step [53/63], Loss: 0.1890\n",
      "Epoch [10/1], Step [54/63], Loss: 0.1551\n",
      "Epoch [10/1], Step [55/63], Loss: 0.1039\n",
      "Epoch [10/1], Step [56/63], Loss: 0.1121\n",
      "Epoch [10/1], Step [57/63], Loss: 0.1314\n",
      "Epoch [10/1], Step [58/63], Loss: 0.0995\n",
      "Epoch [10/1], Step [59/63], Loss: 0.1372\n",
      "Epoch [10/1], Step [60/63], Loss: 0.1032\n",
      "Epoch [10/1], Step [61/63], Loss: 0.1354\n",
      "Epoch [10/1], Step [62/63], Loss: 0.1618\n",
      "Epoch [10/1], Step [63/63], Loss: 0.1603\n",
      "Accuracy: 0.18166039242742849\n",
      "Num Accuracy: 0.5227159261108337\n",
      "Epoch [11/1], Step [1/63], Loss: 0.0698\n",
      "Epoch [11/1], Step [2/63], Loss: 0.0557\n",
      "Epoch [11/1], Step [3/63], Loss: 0.0864\n",
      "Epoch [11/1], Step [4/63], Loss: 0.0769\n",
      "Epoch [11/1], Step [5/63], Loss: 0.0978\n",
      "Epoch [11/1], Step [6/63], Loss: 0.0721\n",
      "Epoch [11/1], Step [7/63], Loss: 0.0385\n",
      "Epoch [11/1], Step [8/63], Loss: 0.0609\n",
      "Epoch [11/1], Step [9/63], Loss: 0.1195\n",
      "Epoch [11/1], Step [10/63], Loss: 0.0911\n",
      "Epoch [11/1], Step [11/63], Loss: 0.0421\n",
      "Epoch [11/1], Step [12/63], Loss: 0.0980\n",
      "Epoch [11/1], Step [13/63], Loss: 0.0842\n",
      "Epoch [11/1], Step [14/63], Loss: 0.1646\n",
      "Epoch [11/1], Step [15/63], Loss: 0.0662\n",
      "Epoch [11/1], Step [16/63], Loss: 0.1538\n",
      "Epoch [11/1], Step [17/63], Loss: 0.1196\n",
      "Epoch [11/1], Step [18/63], Loss: 0.1118\n",
      "Epoch [11/1], Step [19/63], Loss: 0.1094\n",
      "Epoch [11/1], Step [20/63], Loss: 0.0747\n",
      "Epoch [11/1], Step [21/63], Loss: 0.0803\n",
      "Epoch [11/1], Step [22/63], Loss: 0.0924\n",
      "Epoch [11/1], Step [23/63], Loss: 0.0624\n",
      "Epoch [11/1], Step [24/63], Loss: 0.0890\n",
      "Epoch [11/1], Step [25/63], Loss: 0.0463\n",
      "Epoch [11/1], Step [26/63], Loss: 0.1360\n",
      "Epoch [11/1], Step [27/63], Loss: 0.0853\n",
      "Epoch [11/1], Step [28/63], Loss: 0.1089\n",
      "Epoch [11/1], Step [29/63], Loss: 0.1185\n",
      "Epoch [11/1], Step [30/63], Loss: 0.1156\n",
      "Epoch [11/1], Step [31/63], Loss: 0.0513\n",
      "Epoch [11/1], Step [32/63], Loss: 0.1167\n",
      "Epoch [11/1], Step [33/63], Loss: 0.1023\n",
      "Epoch [11/1], Step [34/63], Loss: 0.0871\n",
      "Epoch [11/1], Step [35/63], Loss: 0.0888\n",
      "Epoch [11/1], Step [36/63], Loss: 0.0495\n",
      "Epoch [11/1], Step [37/63], Loss: 0.1003\n",
      "Epoch [11/1], Step [38/63], Loss: 0.0758\n",
      "Epoch [11/1], Step [39/63], Loss: 0.0754\n",
      "Epoch [11/1], Step [40/63], Loss: 0.1182\n",
      "Epoch [11/1], Step [41/63], Loss: 0.1130\n",
      "Epoch [11/1], Step [42/63], Loss: 0.1285\n",
      "Epoch [11/1], Step [43/63], Loss: 0.0736\n",
      "Epoch [11/1], Step [44/63], Loss: 0.0690\n",
      "Epoch [11/1], Step [45/63], Loss: 0.2009\n",
      "Epoch [11/1], Step [46/63], Loss: 0.1067\n",
      "Epoch [11/1], Step [47/63], Loss: 0.0872\n",
      "Epoch [11/1], Step [48/63], Loss: 0.0670\n",
      "Epoch [11/1], Step [49/63], Loss: 0.0934\n",
      "Epoch [11/1], Step [50/63], Loss: 0.1126\n",
      "Epoch [11/1], Step [51/63], Loss: 0.1007\n",
      "Epoch [11/1], Step [52/63], Loss: 0.1068\n",
      "Epoch [11/1], Step [53/63], Loss: 0.0793\n",
      "Epoch [11/1], Step [54/63], Loss: 0.0886\n",
      "Epoch [11/1], Step [55/63], Loss: 0.0774\n",
      "Epoch [11/1], Step [56/63], Loss: 0.0745\n",
      "Epoch [11/1], Step [57/63], Loss: 0.0739\n",
      "Epoch [11/1], Step [58/63], Loss: 0.0861\n",
      "Epoch [11/1], Step [59/63], Loss: 0.1261\n",
      "Epoch [11/1], Step [60/63], Loss: 0.0770\n",
      "Epoch [11/1], Step [61/63], Loss: 0.1415\n",
      "Epoch [11/1], Step [62/63], Loss: 0.0841\n",
      "Epoch [11/1], Step [63/63], Loss: 0.0800\n",
      "Accuracy: 0.19122052029374176\n",
      "Num Accuracy: 0.49326010983524715\n",
      "Epoch [12/1], Step [1/63], Loss: 0.0692\n",
      "Epoch [12/1], Step [2/63], Loss: 0.0902\n",
      "Epoch [12/1], Step [3/63], Loss: 0.1002\n",
      "Epoch [12/1], Step [4/63], Loss: 0.0702\n",
      "Epoch [12/1], Step [5/63], Loss: 0.0944\n",
      "Epoch [12/1], Step [6/63], Loss: 0.0984\n",
      "Epoch [12/1], Step [7/63], Loss: 0.0832\n",
      "Epoch [12/1], Step [8/63], Loss: 0.1026\n",
      "Epoch [12/1], Step [9/63], Loss: 0.0829\n",
      "Epoch [12/1], Step [10/63], Loss: 0.0464\n",
      "Epoch [12/1], Step [11/63], Loss: 0.0811\n",
      "Epoch [12/1], Step [12/63], Loss: 0.0407\n",
      "Epoch [12/1], Step [13/63], Loss: 0.0958\n",
      "Epoch [12/1], Step [14/63], Loss: 0.1090\n",
      "Epoch [12/1], Step [15/63], Loss: 0.0551\n",
      "Epoch [12/1], Step [16/63], Loss: 0.0938\n",
      "Epoch [12/1], Step [17/63], Loss: 0.1262\n",
      "Epoch [12/1], Step [18/63], Loss: 0.0820\n",
      "Epoch [12/1], Step [19/63], Loss: 0.0470\n",
      "Epoch [12/1], Step [20/63], Loss: 0.1029\n",
      "Epoch [12/1], Step [21/63], Loss: 0.0732\n",
      "Epoch [12/1], Step [22/63], Loss: 0.0826\n",
      "Epoch [12/1], Step [23/63], Loss: 0.0947\n",
      "Epoch [12/1], Step [24/63], Loss: 0.0920\n",
      "Epoch [12/1], Step [25/63], Loss: 0.0665\n",
      "Epoch [12/1], Step [26/63], Loss: 0.1074\n",
      "Epoch [12/1], Step [27/63], Loss: 0.0923\n",
      "Epoch [12/1], Step [28/63], Loss: 0.0842\n",
      "Epoch [12/1], Step [29/63], Loss: 0.0888\n",
      "Epoch [12/1], Step [30/63], Loss: 0.0806\n",
      "Epoch [12/1], Step [31/63], Loss: 0.0596\n",
      "Epoch [12/1], Step [32/63], Loss: 0.0934\n",
      "Epoch [12/1], Step [33/63], Loss: 0.1184\n",
      "Epoch [12/1], Step [34/63], Loss: 0.0529\n",
      "Epoch [12/1], Step [35/63], Loss: 0.0482\n",
      "Epoch [12/1], Step [36/63], Loss: 0.0856\n",
      "Epoch [12/1], Step [37/63], Loss: 0.1099\n",
      "Epoch [12/1], Step [38/63], Loss: 0.0758\n",
      "Epoch [12/1], Step [39/63], Loss: 0.1128\n",
      "Epoch [12/1], Step [40/63], Loss: 0.0781\n",
      "Epoch [12/1], Step [41/63], Loss: 0.0717\n",
      "Epoch [12/1], Step [42/63], Loss: 0.0864\n",
      "Epoch [12/1], Step [43/63], Loss: 0.1000\n",
      "Epoch [12/1], Step [44/63], Loss: 0.1000\n",
      "Epoch [12/1], Step [45/63], Loss: 0.0890\n",
      "Epoch [12/1], Step [46/63], Loss: 0.0632\n",
      "Epoch [12/1], Step [47/63], Loss: 0.1123\n",
      "Epoch [12/1], Step [48/63], Loss: 0.1261\n",
      "Epoch [12/1], Step [49/63], Loss: 0.0702\n",
      "Epoch [12/1], Step [50/63], Loss: 0.0765\n",
      "Epoch [12/1], Step [51/63], Loss: 0.0820\n",
      "Epoch [12/1], Step [52/63], Loss: 0.0576\n",
      "Epoch [12/1], Step [53/63], Loss: 0.1622\n",
      "Epoch [12/1], Step [54/63], Loss: 0.0895\n",
      "Epoch [12/1], Step [55/63], Loss: 0.0772\n",
      "Epoch [12/1], Step [56/63], Loss: 0.0704\n",
      "Epoch [12/1], Step [57/63], Loss: 0.0754\n",
      "Epoch [12/1], Step [58/63], Loss: 0.1028\n",
      "Epoch [12/1], Step [59/63], Loss: 0.0716\n",
      "Epoch [12/1], Step [60/63], Loss: 0.0927\n",
      "Epoch [12/1], Step [61/63], Loss: 0.0484\n",
      "Epoch [12/1], Step [62/63], Loss: 0.0827\n",
      "Epoch [12/1], Step [63/63], Loss: 0.0676\n",
      "Accuracy: 0.2021669933064982\n",
      "Num Accuracy: 0.5376934598102846\n",
      "Epoch [13/1], Step [1/63], Loss: 0.0328\n",
      "Epoch [13/1], Step [2/63], Loss: 0.0534\n",
      "Epoch [13/1], Step [3/63], Loss: 0.0415\n",
      "Epoch [13/1], Step [4/63], Loss: 0.0730\n",
      "Epoch [13/1], Step [5/63], Loss: 0.0528\n",
      "Epoch [13/1], Step [6/63], Loss: 0.0923\n",
      "Epoch [13/1], Step [7/63], Loss: 0.0530\n",
      "Epoch [13/1], Step [8/63], Loss: 0.0729\n",
      "Epoch [13/1], Step [9/63], Loss: 0.0766\n",
      "Epoch [13/1], Step [10/63], Loss: 0.0758\n",
      "Epoch [13/1], Step [11/63], Loss: 0.0953\n",
      "Epoch [13/1], Step [12/63], Loss: 0.0990\n",
      "Epoch [13/1], Step [13/63], Loss: 0.0811\n",
      "Epoch [13/1], Step [14/63], Loss: 0.0495\n",
      "Epoch [13/1], Step [15/63], Loss: 0.0784\n",
      "Epoch [13/1], Step [16/63], Loss: 0.0484\n",
      "Epoch [13/1], Step [17/63], Loss: 0.0603\n",
      "Epoch [13/1], Step [18/63], Loss: 0.1119\n",
      "Epoch [13/1], Step [19/63], Loss: 0.0527\n",
      "Epoch [13/1], Step [20/63], Loss: 0.0754\n",
      "Epoch [13/1], Step [21/63], Loss: 0.0981\n",
      "Epoch [13/1], Step [22/63], Loss: 0.0392\n",
      "Epoch [13/1], Step [23/63], Loss: 0.0986\n",
      "Epoch [13/1], Step [24/63], Loss: 0.0745\n",
      "Epoch [13/1], Step [25/63], Loss: 0.0604\n",
      "Epoch [13/1], Step [26/63], Loss: 0.1053\n",
      "Epoch [13/1], Step [27/63], Loss: 0.0644\n",
      "Epoch [13/1], Step [28/63], Loss: 0.0751\n",
      "Epoch [13/1], Step [29/63], Loss: 0.0911\n",
      "Epoch [13/1], Step [30/63], Loss: 0.1144\n",
      "Epoch [13/1], Step [31/63], Loss: 0.0816\n",
      "Epoch [13/1], Step [32/63], Loss: 0.0901\n",
      "Epoch [13/1], Step [33/63], Loss: 0.0723\n",
      "Epoch [13/1], Step [34/63], Loss: 0.0661\n",
      "Epoch [13/1], Step [35/63], Loss: 0.0662\n",
      "Epoch [13/1], Step [36/63], Loss: 0.0826\n",
      "Epoch [13/1], Step [37/63], Loss: 0.0553\n",
      "Epoch [13/1], Step [38/63], Loss: 0.0702\n",
      "Epoch [13/1], Step [39/63], Loss: 0.1008\n",
      "Epoch [13/1], Step [40/63], Loss: 0.0453\n",
      "Epoch [13/1], Step [41/63], Loss: 0.0701\n",
      "Epoch [13/1], Step [42/63], Loss: 0.1263\n",
      "Epoch [13/1], Step [43/63], Loss: 0.0486\n",
      "Epoch [13/1], Step [44/63], Loss: 0.1446\n",
      "Epoch [13/1], Step [45/63], Loss: 0.0598\n",
      "Epoch [13/1], Step [46/63], Loss: 0.0562\n",
      "Epoch [13/1], Step [47/63], Loss: 0.0800\n",
      "Epoch [13/1], Step [48/63], Loss: 0.0573\n",
      "Epoch [13/1], Step [49/63], Loss: 0.0766\n",
      "Epoch [13/1], Step [50/63], Loss: 0.0856\n",
      "Epoch [13/1], Step [51/63], Loss: 0.0878\n",
      "Epoch [13/1], Step [52/63], Loss: 0.1735\n",
      "Epoch [13/1], Step [53/63], Loss: 0.0707\n",
      "Epoch [13/1], Step [54/63], Loss: 0.0696\n",
      "Epoch [13/1], Step [55/63], Loss: 0.1286\n",
      "Epoch [13/1], Step [56/63], Loss: 0.0671\n",
      "Epoch [13/1], Step [57/63], Loss: 0.0817\n",
      "Epoch [13/1], Step [58/63], Loss: 0.1152\n",
      "Epoch [13/1], Step [59/63], Loss: 0.1222\n",
      "Epoch [13/1], Step [60/63], Loss: 0.1070\n",
      "Epoch [13/1], Step [61/63], Loss: 0.1006\n",
      "Epoch [13/1], Step [62/63], Loss: 0.0834\n",
      "Epoch [13/1], Step [63/63], Loss: 0.0623\n",
      "Accuracy: 0.18749065505910784\n",
      "Num Accuracy: 0.5057413879181228\n",
      "Epoch [14/1], Step [1/63], Loss: 0.0779\n",
      "Epoch [14/1], Step [2/63], Loss: 0.0547\n",
      "Epoch [14/1], Step [3/63], Loss: 0.0395\n",
      "Epoch [14/1], Step [4/63], Loss: 0.0337\n",
      "Epoch [14/1], Step [5/63], Loss: 0.0717\n",
      "Epoch [14/1], Step [6/63], Loss: 0.0616\n",
      "Epoch [14/1], Step [7/63], Loss: 0.0572\n",
      "Epoch [14/1], Step [8/63], Loss: 0.0577\n",
      "Epoch [14/1], Step [9/63], Loss: 0.0580\n",
      "Epoch [14/1], Step [10/63], Loss: 0.0622\n",
      "Epoch [14/1], Step [11/63], Loss: 0.0655\n",
      "Epoch [14/1], Step [12/63], Loss: 0.0882\n",
      "Epoch [14/1], Step [13/63], Loss: 0.0461\n",
      "Epoch [14/1], Step [14/63], Loss: 0.0766\n",
      "Epoch [14/1], Step [15/63], Loss: 0.1172\n",
      "Epoch [14/1], Step [16/63], Loss: 0.0885\n",
      "Epoch [14/1], Step [17/63], Loss: 0.0918\n",
      "Epoch [14/1], Step [18/63], Loss: 0.0604\n",
      "Epoch [14/1], Step [19/63], Loss: 0.0946\n",
      "Epoch [14/1], Step [20/63], Loss: 0.0449\n",
      "Epoch [14/1], Step [21/63], Loss: 0.0424\n",
      "Epoch [14/1], Step [22/63], Loss: 0.0660\n",
      "Epoch [14/1], Step [23/63], Loss: 0.0672\n",
      "Epoch [14/1], Step [24/63], Loss: 0.0709\n",
      "Epoch [14/1], Step [25/63], Loss: 0.0421\n",
      "Epoch [14/1], Step [26/63], Loss: 0.0733\n",
      "Epoch [14/1], Step [27/63], Loss: 0.0495\n",
      "Epoch [14/1], Step [28/63], Loss: 0.0542\n",
      "Epoch [14/1], Step [29/63], Loss: 0.0904\n",
      "Epoch [14/1], Step [30/63], Loss: 0.0630\n",
      "Epoch [14/1], Step [31/63], Loss: 0.0824\n",
      "Epoch [14/1], Step [32/63], Loss: 0.1164\n",
      "Epoch [14/1], Step [33/63], Loss: 0.0707\n",
      "Epoch [14/1], Step [34/63], Loss: 0.0606\n",
      "Epoch [14/1], Step [35/63], Loss: 0.0629\n",
      "Epoch [14/1], Step [36/63], Loss: 0.0712\n",
      "Epoch [14/1], Step [37/63], Loss: 0.0806\n",
      "Epoch [14/1], Step [38/63], Loss: 0.0884\n",
      "Epoch [14/1], Step [39/63], Loss: 0.0858\n",
      "Epoch [14/1], Step [40/63], Loss: 0.0787\n",
      "Epoch [14/1], Step [41/63], Loss: 0.0458\n",
      "Epoch [14/1], Step [42/63], Loss: 0.0441\n",
      "Epoch [14/1], Step [43/63], Loss: 0.1005\n",
      "Epoch [14/1], Step [44/63], Loss: 0.1234\n",
      "Epoch [14/1], Step [45/63], Loss: 0.0350\n",
      "Epoch [14/1], Step [46/63], Loss: 0.1084\n",
      "Epoch [14/1], Step [47/63], Loss: 0.1039\n",
      "Epoch [14/1], Step [48/63], Loss: 0.0517\n",
      "Epoch [14/1], Step [49/63], Loss: 0.0918\n",
      "Epoch [14/1], Step [50/63], Loss: 0.0784\n",
      "Epoch [14/1], Step [51/63], Loss: 0.0642\n",
      "Epoch [14/1], Step [52/63], Loss: 0.0683\n",
      "Epoch [14/1], Step [53/63], Loss: 0.0816\n",
      "Epoch [14/1], Step [54/63], Loss: 0.1400\n",
      "Epoch [14/1], Step [55/63], Loss: 0.0847\n",
      "Epoch [14/1], Step [56/63], Loss: 0.1020\n",
      "Epoch [14/1], Step [57/63], Loss: 0.0918\n",
      "Epoch [14/1], Step [58/63], Loss: 0.0630\n",
      "Epoch [14/1], Step [59/63], Loss: 0.0942\n",
      "Epoch [14/1], Step [60/63], Loss: 0.0635\n",
      "Epoch [14/1], Step [61/63], Loss: 0.0378\n",
      "Epoch [14/1], Step [62/63], Loss: 0.0962\n",
      "Epoch [14/1], Step [63/63], Loss: 0.1074\n",
      "Accuracy: 0.21117774454399804\n",
      "Num Accuracy: 0.5436844732900649\n",
      "Epoch [15/1], Step [1/63], Loss: 0.0601\n",
      "Epoch [15/1], Step [2/63], Loss: 0.0776\n",
      "Epoch [15/1], Step [3/63], Loss: 0.0591\n",
      "Epoch [15/1], Step [4/63], Loss: 0.0433\n",
      "Epoch [15/1], Step [5/63], Loss: 0.1231\n",
      "Epoch [15/1], Step [6/63], Loss: 0.0417\n",
      "Epoch [15/1], Step [7/63], Loss: 0.0599\n",
      "Epoch [15/1], Step [8/63], Loss: 0.1099\n",
      "Epoch [15/1], Step [9/63], Loss: 0.1006\n",
      "Epoch [15/1], Step [10/63], Loss: 0.0598\n",
      "Epoch [15/1], Step [11/63], Loss: 0.0314\n",
      "Epoch [15/1], Step [12/63], Loss: 0.0281\n",
      "Epoch [15/1], Step [13/63], Loss: 0.0892\n",
      "Epoch [15/1], Step [14/63], Loss: 0.1232\n",
      "Epoch [15/1], Step [15/63], Loss: 0.0454\n",
      "Epoch [15/1], Step [16/63], Loss: 0.0962\n",
      "Epoch [15/1], Step [17/63], Loss: 0.0826\n",
      "Epoch [15/1], Step [18/63], Loss: 0.0588\n",
      "Epoch [15/1], Step [19/63], Loss: 0.1199\n",
      "Epoch [15/1], Step [20/63], Loss: 0.1095\n",
      "Epoch [15/1], Step [21/63], Loss: 0.0533\n",
      "Epoch [15/1], Step [22/63], Loss: 0.0612\n",
      "Epoch [15/1], Step [23/63], Loss: 0.0415\n",
      "Epoch [15/1], Step [24/63], Loss: 0.0589\n",
      "Epoch [15/1], Step [25/63], Loss: 0.0608\n",
      "Epoch [15/1], Step [26/63], Loss: 0.0574\n",
      "Epoch [15/1], Step [27/63], Loss: 0.0492\n",
      "Epoch [15/1], Step [28/63], Loss: 0.0422\n",
      "Epoch [15/1], Step [29/63], Loss: 0.0415\n",
      "Epoch [15/1], Step [30/63], Loss: 0.0444\n",
      "Epoch [15/1], Step [31/63], Loss: 0.0562\n",
      "Epoch [15/1], Step [32/63], Loss: 0.0552\n",
      "Epoch [15/1], Step [33/63], Loss: 0.0536\n",
      "Epoch [15/1], Step [34/63], Loss: 0.0728\n",
      "Epoch [15/1], Step [35/63], Loss: 0.0710\n",
      "Epoch [15/1], Step [36/63], Loss: 0.0468\n",
      "Epoch [15/1], Step [37/63], Loss: 0.1158\n",
      "Epoch [15/1], Step [38/63], Loss: 0.0981\n",
      "Epoch [15/1], Step [39/63], Loss: 0.0207\n",
      "Epoch [15/1], Step [40/63], Loss: 0.0969\n",
      "Epoch [15/1], Step [41/63], Loss: 0.1125\n",
      "Epoch [15/1], Step [42/63], Loss: 0.0460\n",
      "Epoch [15/1], Step [43/63], Loss: 0.0839\n",
      "Epoch [15/1], Step [44/63], Loss: 0.0314\n",
      "Epoch [15/1], Step [45/63], Loss: 0.0236\n",
      "Epoch [15/1], Step [46/63], Loss: 0.0980\n",
      "Epoch [15/1], Step [47/63], Loss: 0.0667\n",
      "Epoch [15/1], Step [48/63], Loss: 0.0724\n",
      "Epoch [15/1], Step [49/63], Loss: 0.0932\n",
      "Epoch [15/1], Step [50/63], Loss: 0.0301\n",
      "Epoch [15/1], Step [51/63], Loss: 0.0816\n",
      "Epoch [15/1], Step [52/63], Loss: 0.1038\n",
      "Epoch [15/1], Step [53/63], Loss: 0.0648\n",
      "Epoch [15/1], Step [54/63], Loss: 0.0633\n",
      "Epoch [15/1], Step [55/63], Loss: 0.0478\n",
      "Epoch [15/1], Step [56/63], Loss: 0.0986\n",
      "Epoch [15/1], Step [57/63], Loss: 0.0689\n",
      "Epoch [15/1], Step [58/63], Loss: 0.0516\n",
      "Epoch [15/1], Step [59/63], Loss: 0.0900\n",
      "Epoch [15/1], Step [60/63], Loss: 0.1276\n",
      "Epoch [15/1], Step [61/63], Loss: 0.0857\n",
      "Epoch [15/1], Step [62/63], Loss: 0.0692\n",
      "Epoch [15/1], Step [63/63], Loss: 0.1916\n",
      "Accuracy: 0.19643004363667246\n",
      "Num Accuracy: 0.5521717423864204\n",
      "Epoch [16/1], Step [1/63], Loss: 0.0422\n",
      "Epoch [16/1], Step [2/63], Loss: 0.0485\n",
      "Epoch [16/1], Step [3/63], Loss: 0.0583\n",
      "Epoch [16/1], Step [4/63], Loss: 0.1312\n",
      "Epoch [16/1], Step [5/63], Loss: 0.0884\n",
      "Epoch [16/1], Step [6/63], Loss: 0.0512\n",
      "Epoch [16/1], Step [7/63], Loss: 0.0468\n",
      "Epoch [16/1], Step [8/63], Loss: 0.0352\n",
      "Epoch [16/1], Step [9/63], Loss: 0.0964\n",
      "Epoch [16/1], Step [10/63], Loss: 0.0531\n",
      "Epoch [16/1], Step [11/63], Loss: 0.1099\n",
      "Epoch [16/1], Step [12/63], Loss: 0.0513\n",
      "Epoch [16/1], Step [13/63], Loss: 0.0368\n",
      "Epoch [16/1], Step [14/63], Loss: 0.0926\n",
      "Epoch [16/1], Step [15/63], Loss: 0.0942\n",
      "Epoch [16/1], Step [16/63], Loss: 0.0577\n",
      "Epoch [16/1], Step [17/63], Loss: 0.0813\n",
      "Epoch [16/1], Step [18/63], Loss: 0.0755\n",
      "Epoch [16/1], Step [19/63], Loss: 0.0706\n",
      "Epoch [16/1], Step [20/63], Loss: 0.1261\n",
      "Epoch [16/1], Step [21/63], Loss: 0.0616\n",
      "Epoch [16/1], Step [22/63], Loss: 0.0426\n",
      "Epoch [16/1], Step [23/63], Loss: 0.0676\n",
      "Epoch [16/1], Step [24/63], Loss: 0.0475\n",
      "Epoch [16/1], Step [25/63], Loss: 0.1135\n",
      "Epoch [16/1], Step [26/63], Loss: 0.0856\n",
      "Epoch [16/1], Step [27/63], Loss: 0.0930\n",
      "Epoch [16/1], Step [28/63], Loss: 0.0483\n",
      "Epoch [16/1], Step [29/63], Loss: 0.0618\n",
      "Epoch [16/1], Step [30/63], Loss: 0.0895\n",
      "Epoch [16/1], Step [31/63], Loss: 0.0722\n",
      "Epoch [16/1], Step [32/63], Loss: 0.0709\n",
      "Epoch [16/1], Step [33/63], Loss: 0.0628\n",
      "Epoch [16/1], Step [34/63], Loss: 0.1161\n",
      "Epoch [16/1], Step [35/63], Loss: 0.0467\n",
      "Epoch [16/1], Step [36/63], Loss: 0.1097\n",
      "Epoch [16/1], Step [37/63], Loss: 0.1078\n",
      "Epoch [16/1], Step [38/63], Loss: 0.0385\n",
      "Epoch [16/1], Step [39/63], Loss: 0.1185\n",
      "Epoch [16/1], Step [40/63], Loss: 0.1345\n",
      "Epoch [16/1], Step [41/63], Loss: 0.0877\n",
      "Epoch [16/1], Step [42/63], Loss: 0.0472\n",
      "Epoch [16/1], Step [43/63], Loss: 0.1204\n",
      "Epoch [16/1], Step [44/63], Loss: 0.0802\n",
      "Epoch [16/1], Step [45/63], Loss: 0.0771\n",
      "Epoch [16/1], Step [46/63], Loss: 0.0748\n",
      "Epoch [16/1], Step [47/63], Loss: 0.0316\n",
      "Epoch [16/1], Step [48/63], Loss: 0.0308\n",
      "Epoch [16/1], Step [49/63], Loss: 0.1002\n",
      "Epoch [16/1], Step [50/63], Loss: 0.0892\n",
      "Epoch [16/1], Step [51/63], Loss: 0.1089\n",
      "Epoch [16/1], Step [52/63], Loss: 0.0548\n",
      "Epoch [16/1], Step [53/63], Loss: 0.0750\n",
      "Epoch [16/1], Step [54/63], Loss: 0.0699\n",
      "Epoch [16/1], Step [55/63], Loss: 0.0640\n",
      "Epoch [16/1], Step [56/63], Loss: 0.0831\n",
      "Epoch [16/1], Step [57/63], Loss: 0.0439\n",
      "Epoch [16/1], Step [58/63], Loss: 0.1305\n",
      "Epoch [16/1], Step [59/63], Loss: 0.0713\n",
      "Epoch [16/1], Step [60/63], Loss: 0.0873\n",
      "Epoch [16/1], Step [61/63], Loss: 0.1116\n",
      "Epoch [16/1], Step [62/63], Loss: 0.0605\n",
      "Epoch [16/1], Step [63/63], Loss: 0.0359\n",
      "Accuracy: 0.18135716993376513\n",
      "Num Accuracy: 0.5172241637543684\n",
      "Epoch [17/1], Step [1/63], Loss: 0.0935\n",
      "Epoch [17/1], Step [2/63], Loss: 0.0478\n",
      "Epoch [17/1], Step [3/63], Loss: 0.0740\n",
      "Epoch [17/1], Step [4/63], Loss: 0.0631\n",
      "Epoch [17/1], Step [5/63], Loss: 0.0749\n",
      "Epoch [17/1], Step [6/63], Loss: 0.0500\n",
      "Epoch [17/1], Step [7/63], Loss: 0.0822\n",
      "Epoch [17/1], Step [8/63], Loss: 0.0706\n",
      "Epoch [17/1], Step [9/63], Loss: 0.0459\n",
      "Epoch [17/1], Step [10/63], Loss: 0.0272\n",
      "Epoch [17/1], Step [11/63], Loss: 0.0434\n",
      "Epoch [17/1], Step [12/63], Loss: 0.0458\n",
      "Epoch [17/1], Step [13/63], Loss: 0.0367\n",
      "Epoch [17/1], Step [14/63], Loss: 0.0680\n",
      "Epoch [17/1], Step [15/63], Loss: 0.0377\n",
      "Epoch [17/1], Step [16/63], Loss: 0.0877\n",
      "Epoch [17/1], Step [17/63], Loss: 0.0443\n",
      "Epoch [17/1], Step [18/63], Loss: 0.0446\n",
      "Epoch [17/1], Step [19/63], Loss: 0.0789\n",
      "Epoch [17/1], Step [20/63], Loss: 0.0665\n",
      "Epoch [17/1], Step [21/63], Loss: 0.0713\n",
      "Epoch [17/1], Step [22/63], Loss: 0.0295\n",
      "Epoch [17/1], Step [23/63], Loss: 0.0360\n",
      "Epoch [17/1], Step [24/63], Loss: 0.0616\n",
      "Epoch [17/1], Step [25/63], Loss: 0.0428\n",
      "Epoch [17/1], Step [26/63], Loss: 0.0714\n",
      "Epoch [17/1], Step [27/63], Loss: 0.0374\n",
      "Epoch [17/1], Step [28/63], Loss: 0.1609\n",
      "Epoch [17/1], Step [29/63], Loss: 0.0539\n",
      "Epoch [17/1], Step [30/63], Loss: 0.0631\n",
      "Epoch [17/1], Step [31/63], Loss: 0.0673\n",
      "Epoch [17/1], Step [32/63], Loss: 0.0687\n",
      "Epoch [17/1], Step [33/63], Loss: 0.0405\n",
      "Epoch [17/1], Step [34/63], Loss: 0.0748\n",
      "Epoch [17/1], Step [35/63], Loss: 0.0638\n",
      "Epoch [17/1], Step [36/63], Loss: 0.0933\n",
      "Epoch [17/1], Step [37/63], Loss: 0.0340\n",
      "Epoch [17/1], Step [38/63], Loss: 0.0750\n",
      "Epoch [17/1], Step [39/63], Loss: 0.0658\n",
      "Epoch [17/1], Step [40/63], Loss: 0.1187\n",
      "Epoch [17/1], Step [41/63], Loss: 0.0300\n",
      "Epoch [17/1], Step [42/63], Loss: 0.0611\n",
      "Epoch [17/1], Step [43/63], Loss: 0.0575\n",
      "Epoch [17/1], Step [44/63], Loss: 0.0949\n",
      "Epoch [17/1], Step [45/63], Loss: 0.0846\n",
      "Epoch [17/1], Step [46/63], Loss: 0.0869\n",
      "Epoch [17/1], Step [47/63], Loss: 0.0560\n",
      "Epoch [17/1], Step [48/63], Loss: 0.0865\n",
      "Epoch [17/1], Step [49/63], Loss: 0.0655\n",
      "Epoch [17/1], Step [50/63], Loss: 0.0470\n",
      "Epoch [17/1], Step [51/63], Loss: 0.0438\n",
      "Epoch [17/1], Step [52/63], Loss: 0.0321\n",
      "Epoch [17/1], Step [53/63], Loss: 0.0717\n",
      "Epoch [17/1], Step [54/63], Loss: 0.0603\n",
      "Epoch [17/1], Step [55/63], Loss: 0.0568\n",
      "Epoch [17/1], Step [56/63], Loss: 0.0504\n",
      "Epoch [17/1], Step [57/63], Loss: 0.0646\n",
      "Epoch [17/1], Step [58/63], Loss: 0.0440\n",
      "Epoch [17/1], Step [59/63], Loss: 0.0367\n",
      "Epoch [17/1], Step [60/63], Loss: 0.0457\n",
      "Epoch [17/1], Step [61/63], Loss: 0.0713\n",
      "Epoch [17/1], Step [62/63], Loss: 0.0405\n",
      "Epoch [17/1], Step [63/63], Loss: 0.0415\n",
      "Accuracy: 0.1958380418304703\n",
      "Num Accuracy: 0.54717923115327\n",
      "Epoch [18/1], Step [1/63], Loss: 0.0268\n",
      "Epoch [18/1], Step [2/63], Loss: 0.0320\n",
      "Epoch [18/1], Step [3/63], Loss: 0.0626\n",
      "Epoch [18/1], Step [4/63], Loss: 0.0653\n",
      "Epoch [18/1], Step [5/63], Loss: 0.0872\n",
      "Epoch [18/1], Step [6/63], Loss: 0.0299\n",
      "Epoch [18/1], Step [7/63], Loss: 0.0674\n",
      "Epoch [18/1], Step [8/63], Loss: 0.0283\n",
      "Epoch [18/1], Step [9/63], Loss: 0.0389\n",
      "Epoch [18/1], Step [10/63], Loss: 0.0543\n",
      "Epoch [18/1], Step [11/63], Loss: 0.0612\n",
      "Epoch [18/1], Step [12/63], Loss: 0.0379\n",
      "Epoch [18/1], Step [13/63], Loss: 0.0228\n",
      "Epoch [18/1], Step [14/63], Loss: 0.0147\n",
      "Epoch [18/1], Step [15/63], Loss: 0.0503\n",
      "Epoch [18/1], Step [16/63], Loss: 0.0497\n",
      "Epoch [18/1], Step [17/63], Loss: 0.0655\n",
      "Epoch [18/1], Step [18/63], Loss: 0.0800\n",
      "Epoch [18/1], Step [19/63], Loss: 0.0435\n",
      "Epoch [18/1], Step [20/63], Loss: 0.0547\n",
      "Epoch [18/1], Step [21/63], Loss: 0.0418\n",
      "Epoch [18/1], Step [22/63], Loss: 0.0320\n",
      "Epoch [18/1], Step [23/63], Loss: 0.0219\n",
      "Epoch [18/1], Step [24/63], Loss: 0.0452\n",
      "Epoch [18/1], Step [25/63], Loss: 0.0398\n",
      "Epoch [18/1], Step [26/63], Loss: 0.0569\n",
      "Epoch [18/1], Step [27/63], Loss: 0.0540\n",
      "Epoch [18/1], Step [28/63], Loss: 0.0517\n",
      "Epoch [18/1], Step [29/63], Loss: 0.0263\n",
      "Epoch [18/1], Step [30/63], Loss: 0.1124\n",
      "Epoch [18/1], Step [31/63], Loss: 0.0596\n",
      "Epoch [18/1], Step [32/63], Loss: 0.0583\n",
      "Epoch [18/1], Step [33/63], Loss: 0.0334\n",
      "Epoch [18/1], Step [34/63], Loss: 0.0443\n",
      "Epoch [18/1], Step [35/63], Loss: 0.0628\n",
      "Epoch [18/1], Step [36/63], Loss: 0.0232\n",
      "Epoch [18/1], Step [37/63], Loss: 0.0174\n",
      "Epoch [18/1], Step [38/63], Loss: 0.0251\n",
      "Epoch [18/1], Step [39/63], Loss: 0.0416\n",
      "Epoch [18/1], Step [40/63], Loss: 0.0464\n",
      "Epoch [18/1], Step [41/63], Loss: 0.0676\n",
      "Epoch [18/1], Step [42/63], Loss: 0.0885\n",
      "Epoch [18/1], Step [43/63], Loss: 0.0567\n",
      "Epoch [18/1], Step [44/63], Loss: 0.0841\n",
      "Epoch [18/1], Step [45/63], Loss: 0.0748\n",
      "Epoch [18/1], Step [46/63], Loss: 0.0654\n",
      "Epoch [18/1], Step [47/63], Loss: 0.0311\n",
      "Epoch [18/1], Step [48/63], Loss: 0.0460\n",
      "Epoch [18/1], Step [49/63], Loss: 0.0487\n",
      "Epoch [18/1], Step [50/63], Loss: 0.0494\n",
      "Epoch [18/1], Step [51/63], Loss: 0.0481\n",
      "Epoch [18/1], Step [52/63], Loss: 0.0760\n",
      "Epoch [18/1], Step [53/63], Loss: 0.0273\n",
      "Epoch [18/1], Step [54/63], Loss: 0.0641\n",
      "Epoch [18/1], Step [55/63], Loss: 0.0258\n",
      "Epoch [18/1], Step [56/63], Loss: 0.0352\n",
      "Epoch [18/1], Step [57/63], Loss: 0.0420\n",
      "Epoch [18/1], Step [58/63], Loss: 0.0581\n",
      "Epoch [18/1], Step [59/63], Loss: 0.0511\n",
      "Epoch [18/1], Step [60/63], Loss: 0.0856\n",
      "Epoch [18/1], Step [61/63], Loss: 0.0518\n",
      "Epoch [18/1], Step [62/63], Loss: 0.0283\n",
      "Epoch [18/1], Step [63/63], Loss: 0.0375\n",
      "Accuracy: 0.19841734230141755\n",
      "Num Accuracy: 0.5586620069895157\n",
      "Epoch [19/1], Step [1/63], Loss: 0.0248\n",
      "Epoch [19/1], Step [2/63], Loss: 0.0474\n",
      "Epoch [19/1], Step [3/63], Loss: 0.0338\n",
      "Epoch [19/1], Step [4/63], Loss: 0.0714\n",
      "Epoch [19/1], Step [5/63], Loss: 0.0771\n",
      "Epoch [19/1], Step [6/63], Loss: 0.0552\n",
      "Epoch [19/1], Step [7/63], Loss: 0.0798\n",
      "Epoch [19/1], Step [8/63], Loss: 0.0426\n",
      "Epoch [19/1], Step [9/63], Loss: 0.0268\n",
      "Epoch [19/1], Step [10/63], Loss: 0.0273\n",
      "Epoch [19/1], Step [11/63], Loss: 0.0402\n",
      "Epoch [19/1], Step [12/63], Loss: 0.0185\n",
      "Epoch [19/1], Step [13/63], Loss: 0.0611\n",
      "Epoch [19/1], Step [14/63], Loss: 0.0443\n",
      "Epoch [19/1], Step [15/63], Loss: 0.0423\n",
      "Epoch [19/1], Step [16/63], Loss: 0.0619\n",
      "Epoch [19/1], Step [17/63], Loss: 0.0262\n",
      "Epoch [19/1], Step [18/63], Loss: 0.0774\n",
      "Epoch [19/1], Step [19/63], Loss: 0.0475\n",
      "Epoch [19/1], Step [20/63], Loss: 0.0440\n",
      "Epoch [19/1], Step [21/63], Loss: 0.0387\n",
      "Epoch [19/1], Step [22/63], Loss: 0.0688\n",
      "Epoch [19/1], Step [23/63], Loss: 0.0318\n",
      "Epoch [19/1], Step [24/63], Loss: 0.0231\n",
      "Epoch [19/1], Step [25/63], Loss: 0.0329\n",
      "Epoch [19/1], Step [26/63], Loss: 0.0237\n",
      "Epoch [19/1], Step [27/63], Loss: 0.0570\n",
      "Epoch [19/1], Step [28/63], Loss: 0.0174\n",
      "Epoch [19/1], Step [29/63], Loss: 0.0595\n",
      "Epoch [19/1], Step [30/63], Loss: 0.0767\n",
      "Epoch [19/1], Step [31/63], Loss: 0.0502\n",
      "Epoch [19/1], Step [32/63], Loss: 0.0649\n",
      "Epoch [19/1], Step [33/63], Loss: 0.0253\n",
      "Epoch [19/1], Step [34/63], Loss: 0.0611\n",
      "Epoch [19/1], Step [35/63], Loss: 0.0300\n",
      "Epoch [19/1], Step [36/63], Loss: 0.0611\n",
      "Epoch [19/1], Step [37/63], Loss: 0.0322\n",
      "Epoch [19/1], Step [38/63], Loss: 0.0392\n",
      "Epoch [19/1], Step [39/63], Loss: 0.0327\n",
      "Epoch [19/1], Step [40/63], Loss: 0.0566\n",
      "Epoch [19/1], Step [41/63], Loss: 0.0253\n",
      "Epoch [19/1], Step [42/63], Loss: 0.0301\n",
      "Epoch [19/1], Step [43/63], Loss: 0.0988\n",
      "Epoch [19/1], Step [44/63], Loss: 0.0234\n",
      "Epoch [19/1], Step [45/63], Loss: 0.0093\n",
      "Epoch [19/1], Step [46/63], Loss: 0.0214\n",
      "Epoch [19/1], Step [47/63], Loss: 0.0686\n",
      "Epoch [19/1], Step [48/63], Loss: 0.0504\n",
      "Epoch [19/1], Step [49/63], Loss: 0.0438\n",
      "Epoch [19/1], Step [50/63], Loss: 0.0474\n",
      "Epoch [19/1], Step [51/63], Loss: 0.0499\n",
      "Epoch [19/1], Step [52/63], Loss: 0.0621\n",
      "Epoch [19/1], Step [53/63], Loss: 0.0806\n",
      "Epoch [19/1], Step [54/63], Loss: 0.0491\n",
      "Epoch [19/1], Step [55/63], Loss: 0.0555\n",
      "Epoch [19/1], Step [56/63], Loss: 0.0621\n",
      "Epoch [19/1], Step [57/63], Loss: 0.0353\n",
      "Epoch [19/1], Step [58/63], Loss: 0.0285\n",
      "Epoch [19/1], Step [59/63], Loss: 0.0249\n",
      "Epoch [19/1], Step [60/63], Loss: 0.0767\n",
      "Epoch [19/1], Step [61/63], Loss: 0.0711\n",
      "Epoch [19/1], Step [62/63], Loss: 0.0673\n",
      "Epoch [19/1], Step [63/63], Loss: 0.0361\n",
      "Accuracy: 0.19060490131088895\n",
      "Num Accuracy: 0.5531702446330504\n",
      "Epoch [20/1], Step [1/63], Loss: 0.0394\n",
      "Epoch [20/1], Step [2/63], Loss: 0.0381\n",
      "Epoch [20/1], Step [3/63], Loss: 0.0486\n",
      "Epoch [20/1], Step [4/63], Loss: 0.0241\n",
      "Epoch [20/1], Step [5/63], Loss: 0.0345\n",
      "Epoch [20/1], Step [6/63], Loss: 0.0277\n",
      "Epoch [20/1], Step [7/63], Loss: 0.0568\n",
      "Epoch [20/1], Step [8/63], Loss: 0.0640\n",
      "Epoch [20/1], Step [9/63], Loss: 0.0265\n",
      "Epoch [20/1], Step [10/63], Loss: 0.0251\n",
      "Epoch [20/1], Step [11/63], Loss: 0.0362\n",
      "Epoch [20/1], Step [12/63], Loss: 0.0467\n",
      "Epoch [20/1], Step [13/63], Loss: 0.0361\n",
      "Epoch [20/1], Step [14/63], Loss: 0.0346\n",
      "Epoch [20/1], Step [15/63], Loss: 0.0381\n",
      "Epoch [20/1], Step [16/63], Loss: 0.0361\n",
      "Epoch [20/1], Step [17/63], Loss: 0.0156\n",
      "Epoch [20/1], Step [18/63], Loss: 0.0692\n",
      "Epoch [20/1], Step [19/63], Loss: 0.0544\n",
      "Epoch [20/1], Step [20/63], Loss: 0.0206\n",
      "Epoch [20/1], Step [21/63], Loss: 0.0938\n",
      "Epoch [20/1], Step [22/63], Loss: 0.0476\n",
      "Epoch [20/1], Step [23/63], Loss: 0.0117\n",
      "Epoch [20/1], Step [24/63], Loss: 0.0188\n",
      "Epoch [20/1], Step [25/63], Loss: 0.0181\n",
      "Epoch [20/1], Step [26/63], Loss: 0.0183\n",
      "Epoch [20/1], Step [27/63], Loss: 0.0527\n",
      "Epoch [20/1], Step [28/63], Loss: 0.0358\n",
      "Epoch [20/1], Step [29/63], Loss: 0.0099\n",
      "Epoch [20/1], Step [30/63], Loss: 0.0441\n",
      "Epoch [20/1], Step [31/63], Loss: 0.0460\n",
      "Epoch [20/1], Step [32/63], Loss: 0.0345\n",
      "Epoch [20/1], Step [33/63], Loss: 0.0654\n",
      "Epoch [20/1], Step [34/63], Loss: 0.0349\n",
      "Epoch [20/1], Step [35/63], Loss: 0.0499\n",
      "Epoch [20/1], Step [36/63], Loss: 0.0730\n",
      "Epoch [20/1], Step [37/63], Loss: 0.0242\n",
      "Epoch [20/1], Step [38/63], Loss: 0.0497\n",
      "Epoch [20/1], Step [39/63], Loss: 0.0638\n",
      "Epoch [20/1], Step [40/63], Loss: 0.0135\n",
      "Epoch [20/1], Step [41/63], Loss: 0.0290\n",
      "Epoch [20/1], Step [42/63], Loss: 0.0236\n",
      "Epoch [20/1], Step [43/63], Loss: 0.0601\n",
      "Epoch [20/1], Step [44/63], Loss: 0.0472\n",
      "Epoch [20/1], Step [45/63], Loss: 0.0805\n",
      "Epoch [20/1], Step [46/63], Loss: 0.0423\n",
      "Epoch [20/1], Step [47/63], Loss: 0.0897\n",
      "Epoch [20/1], Step [48/63], Loss: 0.0504\n",
      "Epoch [20/1], Step [49/63], Loss: 0.0808\n",
      "Epoch [20/1], Step [50/63], Loss: 0.0521\n",
      "Epoch [20/1], Step [51/63], Loss: 0.0194\n",
      "Epoch [20/1], Step [52/63], Loss: 0.0413\n",
      "Epoch [20/1], Step [53/63], Loss: 0.0567\n",
      "Epoch [20/1], Step [54/63], Loss: 0.0664\n",
      "Epoch [20/1], Step [55/63], Loss: 0.0477\n",
      "Epoch [20/1], Step [56/63], Loss: 0.0459\n",
      "Epoch [20/1], Step [57/63], Loss: 0.0818\n",
      "Epoch [20/1], Step [58/63], Loss: 0.0836\n",
      "Epoch [20/1], Step [59/63], Loss: 0.0569\n",
      "Epoch [20/1], Step [60/63], Loss: 0.0650\n",
      "Epoch [20/1], Step [61/63], Loss: 0.0259\n",
      "Epoch [20/1], Step [62/63], Loss: 0.0529\n",
      "Epoch [20/1], Step [63/63], Loss: 0.0154\n",
      "Accuracy: 0.19453969208137015\n",
      "Num Accuracy: 0.5287069395906141\n",
      "Epoch [21/1], Step [1/63], Loss: 0.0107\n",
      "Epoch [21/1], Step [2/63], Loss: 0.0244\n",
      "Epoch [21/1], Step [3/63], Loss: 0.0701\n",
      "Epoch [21/1], Step [4/63], Loss: 0.0349\n",
      "Epoch [21/1], Step [5/63], Loss: 0.0171\n",
      "Epoch [21/1], Step [6/63], Loss: 0.0182\n",
      "Epoch [21/1], Step [7/63], Loss: 0.0303\n",
      "Epoch [21/1], Step [8/63], Loss: 0.0170\n",
      "Epoch [21/1], Step [9/63], Loss: 0.0417\n",
      "Epoch [21/1], Step [10/63], Loss: 0.0435\n",
      "Epoch [21/1], Step [11/63], Loss: 0.0732\n",
      "Epoch [21/1], Step [12/63], Loss: 0.0148\n",
      "Epoch [21/1], Step [13/63], Loss: 0.0292\n",
      "Epoch [21/1], Step [14/63], Loss: 0.0237\n",
      "Epoch [21/1], Step [15/63], Loss: 0.0357\n",
      "Epoch [21/1], Step [16/63], Loss: 0.0522\n",
      "Epoch [21/1], Step [17/63], Loss: 0.0398\n",
      "Epoch [21/1], Step [18/63], Loss: 0.0266\n",
      "Epoch [21/1], Step [19/63], Loss: 0.0167\n",
      "Epoch [21/1], Step [20/63], Loss: 0.0312\n",
      "Epoch [21/1], Step [21/63], Loss: 0.0201\n",
      "Epoch [21/1], Step [22/63], Loss: 0.0375\n",
      "Epoch [21/1], Step [23/63], Loss: 0.0460\n",
      "Epoch [21/1], Step [24/63], Loss: 0.0093\n",
      "Epoch [21/1], Step [25/63], Loss: 0.0258\n",
      "Epoch [21/1], Step [26/63], Loss: 0.0237\n",
      "Epoch [21/1], Step [27/63], Loss: 0.0410\n",
      "Epoch [21/1], Step [28/63], Loss: 0.0145\n",
      "Epoch [21/1], Step [29/63], Loss: 0.0806\n",
      "Epoch [21/1], Step [30/63], Loss: 0.0539\n",
      "Epoch [21/1], Step [31/63], Loss: 0.0409\n",
      "Epoch [21/1], Step [32/63], Loss: 0.0451\n",
      "Epoch [21/1], Step [33/63], Loss: 0.0303\n",
      "Epoch [21/1], Step [34/63], Loss: 0.0491\n",
      "Epoch [21/1], Step [35/63], Loss: 0.0297\n",
      "Epoch [21/1], Step [36/63], Loss: 0.0306\n",
      "Epoch [21/1], Step [37/63], Loss: 0.0375\n",
      "Epoch [21/1], Step [38/63], Loss: 0.0409\n",
      "Epoch [21/1], Step [39/63], Loss: 0.0379\n",
      "Epoch [21/1], Step [40/63], Loss: 0.0480\n",
      "Epoch [21/1], Step [41/63], Loss: 0.0443\n",
      "Epoch [21/1], Step [42/63], Loss: 0.0486\n",
      "Epoch [21/1], Step [43/63], Loss: 0.0211\n",
      "Epoch [21/1], Step [44/63], Loss: 0.0194\n",
      "Epoch [21/1], Step [45/63], Loss: 0.0440\n",
      "Epoch [21/1], Step [46/63], Loss: 0.0213\n",
      "Epoch [21/1], Step [47/63], Loss: 0.0468\n",
      "Epoch [21/1], Step [48/63], Loss: 0.0386\n",
      "Epoch [21/1], Step [49/63], Loss: 0.0674\n",
      "Epoch [21/1], Step [50/63], Loss: 0.0492\n",
      "Epoch [21/1], Step [51/63], Loss: 0.0977\n",
      "Epoch [21/1], Step [52/63], Loss: 0.0180\n",
      "Epoch [21/1], Step [53/63], Loss: 0.0643\n",
      "Epoch [21/1], Step [54/63], Loss: 0.0928\n",
      "Epoch [21/1], Step [55/63], Loss: 0.0227\n",
      "Epoch [21/1], Step [56/63], Loss: 0.0579\n",
      "Epoch [21/1], Step [57/63], Loss: 0.0983\n",
      "Epoch [21/1], Step [58/63], Loss: 0.0643\n",
      "Epoch [21/1], Step [59/63], Loss: 0.0399\n",
      "Epoch [21/1], Step [60/63], Loss: 0.0735\n",
      "Epoch [21/1], Step [61/63], Loss: 0.0485\n",
      "Epoch [21/1], Step [62/63], Loss: 0.0957\n",
      "Epoch [21/1], Step [63/63], Loss: 0.0759\n",
      "Accuracy: 0.2012290729638536\n",
      "Num Accuracy: 0.5571642536195707\n",
      "Epoch [22/1], Step [1/63], Loss: 0.0176\n",
      "Epoch [22/1], Step [2/63], Loss: 0.0188\n",
      "Epoch [22/1], Step [3/63], Loss: 0.0512\n",
      "Epoch [22/1], Step [4/63], Loss: 0.0126\n",
      "Epoch [22/1], Step [5/63], Loss: 0.0552\n",
      "Epoch [22/1], Step [6/63], Loss: 0.0435\n",
      "Epoch [22/1], Step [7/63], Loss: 0.0540\n",
      "Epoch [22/1], Step [8/63], Loss: 0.0509\n",
      "Epoch [22/1], Step [9/63], Loss: 0.0376\n",
      "Epoch [22/1], Step [10/63], Loss: 0.0337\n",
      "Epoch [22/1], Step [11/63], Loss: 0.0100\n",
      "Epoch [22/1], Step [12/63], Loss: 0.0181\n",
      "Epoch [22/1], Step [13/63], Loss: 0.0279\n",
      "Epoch [22/1], Step [14/63], Loss: 0.0465\n",
      "Epoch [22/1], Step [15/63], Loss: 0.0499\n",
      "Epoch [22/1], Step [16/63], Loss: 0.0745\n",
      "Epoch [22/1], Step [17/63], Loss: 0.0480\n",
      "Epoch [22/1], Step [18/63], Loss: 0.0535\n",
      "Epoch [22/1], Step [19/63], Loss: 0.0426\n",
      "Epoch [22/1], Step [20/63], Loss: 0.0357\n",
      "Epoch [22/1], Step [21/63], Loss: 0.0199\n",
      "Epoch [22/1], Step [22/63], Loss: 0.1311\n",
      "Epoch [22/1], Step [23/63], Loss: 0.1268\n",
      "Epoch [22/1], Step [24/63], Loss: 0.0257\n",
      "Epoch [22/1], Step [25/63], Loss: 0.0485\n",
      "Epoch [22/1], Step [26/63], Loss: 0.0457\n",
      "Epoch [22/1], Step [27/63], Loss: 0.0473\n",
      "Epoch [22/1], Step [28/63], Loss: 0.0954\n",
      "Epoch [22/1], Step [29/63], Loss: 0.1187\n",
      "Epoch [22/1], Step [30/63], Loss: 0.1400\n",
      "Epoch [22/1], Step [31/63], Loss: 0.0414\n",
      "Epoch [22/1], Step [32/63], Loss: 0.1278\n",
      "Epoch [22/1], Step [33/63], Loss: 0.0436\n",
      "Epoch [22/1], Step [34/63], Loss: 0.0686\n",
      "Epoch [22/1], Step [35/63], Loss: 0.0360\n",
      "Epoch [22/1], Step [36/63], Loss: 0.1171\n",
      "Epoch [22/1], Step [37/63], Loss: 0.0324\n",
      "Epoch [22/1], Step [38/63], Loss: 0.0160\n",
      "Epoch [22/1], Step [39/63], Loss: 0.0582\n",
      "Epoch [22/1], Step [40/63], Loss: 0.0744\n",
      "Epoch [22/1], Step [41/63], Loss: 0.0208\n",
      "Epoch [22/1], Step [42/63], Loss: 0.0799\n",
      "Epoch [22/1], Step [43/63], Loss: 0.0879\n",
      "Epoch [22/1], Step [44/63], Loss: 0.0700\n",
      "Epoch [22/1], Step [45/63], Loss: 0.0465\n",
      "Epoch [22/1], Step [46/63], Loss: 0.0497\n",
      "Epoch [22/1], Step [47/63], Loss: 0.0343\n",
      "Epoch [22/1], Step [48/63], Loss: 0.0647\n",
      "Epoch [22/1], Step [49/63], Loss: 0.0331\n",
      "Epoch [22/1], Step [50/63], Loss: 0.0415\n",
      "Epoch [22/1], Step [51/63], Loss: 0.0425\n",
      "Epoch [22/1], Step [52/63], Loss: 0.0931\n",
      "Epoch [22/1], Step [53/63], Loss: 0.0692\n",
      "Epoch [22/1], Step [54/63], Loss: 0.0461\n",
      "Epoch [22/1], Step [55/63], Loss: 0.0493\n",
      "Epoch [22/1], Step [56/63], Loss: 0.0469\n",
      "Epoch [22/1], Step [57/63], Loss: 0.0247\n",
      "Epoch [22/1], Step [58/63], Loss: 0.0686\n",
      "Epoch [22/1], Step [59/63], Loss: 0.0713\n",
      "Epoch [22/1], Step [60/63], Loss: 0.0253\n",
      "Epoch [22/1], Step [61/63], Loss: 0.0450\n",
      "Epoch [22/1], Step [62/63], Loss: 0.0381\n",
      "Epoch [22/1], Step [63/63], Loss: 0.0255\n",
      "Accuracy: 0.19627206280448117\n",
      "Num Accuracy: 0.5222166749875187\n",
      "Epoch [23/1], Step [1/63], Loss: 0.0688\n",
      "Epoch [23/1], Step [2/63], Loss: 0.0551\n",
      "Epoch [23/1], Step [3/63], Loss: 0.0300\n",
      "Epoch [23/1], Step [4/63], Loss: 0.0704\n",
      "Epoch [23/1], Step [5/63], Loss: 0.0231\n",
      "Epoch [23/1], Step [6/63], Loss: 0.0307\n",
      "Epoch [23/1], Step [7/63], Loss: 0.0841\n",
      "Epoch [23/1], Step [8/63], Loss: 0.0317\n",
      "Epoch [23/1], Step [9/63], Loss: 0.0202\n",
      "Epoch [23/1], Step [10/63], Loss: 0.1101\n",
      "Epoch [23/1], Step [11/63], Loss: 0.0636\n",
      "Epoch [23/1], Step [12/63], Loss: 0.0161\n",
      "Epoch [23/1], Step [13/63], Loss: 0.0297\n",
      "Epoch [23/1], Step [14/63], Loss: 0.0715\n",
      "Epoch [23/1], Step [15/63], Loss: 0.0672\n",
      "Epoch [23/1], Step [16/63], Loss: 0.0399\n",
      "Epoch [23/1], Step [17/63], Loss: 0.0484\n",
      "Epoch [23/1], Step [18/63], Loss: 0.0939\n",
      "Epoch [23/1], Step [19/63], Loss: 0.1255\n",
      "Epoch [23/1], Step [20/63], Loss: 0.0767\n",
      "Epoch [23/1], Step [21/63], Loss: 0.0394\n",
      "Epoch [23/1], Step [22/63], Loss: 0.0219\n",
      "Epoch [23/1], Step [23/63], Loss: 0.0228\n",
      "Epoch [23/1], Step [24/63], Loss: 0.0365\n",
      "Epoch [23/1], Step [25/63], Loss: 0.0837\n",
      "Epoch [23/1], Step [26/63], Loss: 0.0588\n",
      "Epoch [23/1], Step [27/63], Loss: 0.0494\n",
      "Epoch [23/1], Step [28/63], Loss: 0.0432\n",
      "Epoch [23/1], Step [29/63], Loss: 0.0271\n",
      "Epoch [23/1], Step [30/63], Loss: 0.0436\n",
      "Epoch [23/1], Step [31/63], Loss: 0.0405\n",
      "Epoch [23/1], Step [32/63], Loss: 0.0407\n",
      "Epoch [23/1], Step [33/63], Loss: 0.0145\n",
      "Epoch [23/1], Step [34/63], Loss: 0.0602\n",
      "Epoch [23/1], Step [35/63], Loss: 0.0423\n",
      "Epoch [23/1], Step [36/63], Loss: 0.0789\n",
      "Epoch [23/1], Step [37/63], Loss: 0.0572\n",
      "Epoch [23/1], Step [38/63], Loss: 0.0744\n",
      "Epoch [23/1], Step [39/63], Loss: 0.0277\n",
      "Epoch [23/1], Step [40/63], Loss: 0.1289\n",
      "Epoch [23/1], Step [41/63], Loss: 0.0942\n",
      "Epoch [23/1], Step [42/63], Loss: 0.0461\n",
      "Epoch [23/1], Step [43/63], Loss: 0.0599\n",
      "Epoch [23/1], Step [44/63], Loss: 0.0626\n",
      "Epoch [23/1], Step [45/63], Loss: 0.0593\n",
      "Epoch [23/1], Step [46/63], Loss: 0.0522\n",
      "Epoch [23/1], Step [47/63], Loss: 0.0412\n",
      "Epoch [23/1], Step [48/63], Loss: 0.0255\n",
      "Epoch [23/1], Step [49/63], Loss: 0.0886\n",
      "Epoch [23/1], Step [50/63], Loss: 0.0176\n",
      "Epoch [23/1], Step [51/63], Loss: 0.0523\n",
      "Epoch [23/1], Step [52/63], Loss: 0.0264\n",
      "Epoch [23/1], Step [53/63], Loss: 0.0552\n",
      "Epoch [23/1], Step [54/63], Loss: 0.0291\n",
      "Epoch [23/1], Step [55/63], Loss: 0.0320\n",
      "Epoch [23/1], Step [56/63], Loss: 0.0700\n",
      "Epoch [23/1], Step [57/63], Loss: 0.0625\n",
      "Epoch [23/1], Step [58/63], Loss: 0.0654\n",
      "Epoch [23/1], Step [59/63], Loss: 0.0214\n",
      "Epoch [23/1], Step [60/63], Loss: 0.0512\n",
      "Epoch [23/1], Step [61/63], Loss: 0.0907\n",
      "Epoch [23/1], Step [62/63], Loss: 0.0593\n",
      "Epoch [23/1], Step [63/63], Loss: 0.0263\n",
      "Accuracy: 0.18801944534578857\n",
      "Num Accuracy: 0.5891163255117324\n",
      "Epoch [24/1], Step [1/63], Loss: 0.0305\n",
      "Epoch [24/1], Step [2/63], Loss: 0.0173\n",
      "Epoch [24/1], Step [3/63], Loss: 0.0663\n",
      "Epoch [24/1], Step [4/63], Loss: 0.0533\n",
      "Epoch [24/1], Step [5/63], Loss: 0.0227\n",
      "Epoch [24/1], Step [6/63], Loss: 0.0327\n",
      "Epoch [24/1], Step [7/63], Loss: 0.0787\n",
      "Epoch [24/1], Step [8/63], Loss: 0.0168\n",
      "Epoch [24/1], Step [9/63], Loss: 0.0223\n",
      "Epoch [24/1], Step [10/63], Loss: 0.0178\n",
      "Epoch [24/1], Step [11/63], Loss: 0.0350\n",
      "Epoch [24/1], Step [12/63], Loss: 0.0556\n",
      "Epoch [24/1], Step [13/63], Loss: 0.0339\n",
      "Epoch [24/1], Step [14/63], Loss: 0.0295\n",
      "Epoch [24/1], Step [15/63], Loss: 0.0536\n",
      "Epoch [24/1], Step [16/63], Loss: 0.0285\n",
      "Epoch [24/1], Step [17/63], Loss: 0.0634\n",
      "Epoch [24/1], Step [18/63], Loss: 0.0262\n",
      "Epoch [24/1], Step [19/63], Loss: 0.0296\n",
      "Epoch [24/1], Step [20/63], Loss: 0.0291\n",
      "Epoch [24/1], Step [21/63], Loss: 0.0841\n",
      "Epoch [24/1], Step [22/63], Loss: 0.0254\n",
      "Epoch [24/1], Step [23/63], Loss: 0.0262\n",
      "Epoch [24/1], Step [24/63], Loss: 0.0300\n",
      "Epoch [24/1], Step [25/63], Loss: 0.0626\n",
      "Epoch [24/1], Step [26/63], Loss: 0.0285\n",
      "Epoch [24/1], Step [27/63], Loss: 0.0196\n",
      "Epoch [24/1], Step [28/63], Loss: 0.0325\n",
      "Epoch [24/1], Step [29/63], Loss: 0.0078\n",
      "Epoch [24/1], Step [30/63], Loss: 0.0331\n",
      "Epoch [24/1], Step [31/63], Loss: 0.0271\n",
      "Epoch [24/1], Step [32/63], Loss: 0.0346\n",
      "Epoch [24/1], Step [33/63], Loss: 0.0241\n",
      "Epoch [24/1], Step [34/63], Loss: 0.0396\n",
      "Epoch [24/1], Step [35/63], Loss: 0.0303\n",
      "Epoch [24/1], Step [36/63], Loss: 0.0300\n",
      "Epoch [24/1], Step [37/63], Loss: 0.0270\n",
      "Epoch [24/1], Step [38/63], Loss: 0.0275\n",
      "Epoch [24/1], Step [39/63], Loss: 0.0390\n",
      "Epoch [24/1], Step [40/63], Loss: 0.0333\n",
      "Epoch [24/1], Step [41/63], Loss: 0.0277\n",
      "Epoch [24/1], Step [42/63], Loss: 0.0322\n",
      "Epoch [24/1], Step [43/63], Loss: 0.0248\n",
      "Epoch [24/1], Step [44/63], Loss: 0.0612\n",
      "Epoch [24/1], Step [45/63], Loss: 0.0454\n",
      "Epoch [24/1], Step [46/63], Loss: 0.0194\n",
      "Epoch [24/1], Step [47/63], Loss: 0.0183\n",
      "Epoch [24/1], Step [48/63], Loss: 0.0841\n",
      "Epoch [24/1], Step [49/63], Loss: 0.0234\n",
      "Epoch [24/1], Step [50/63], Loss: 0.0277\n",
      "Epoch [24/1], Step [51/63], Loss: 0.0088\n",
      "Epoch [24/1], Step [52/63], Loss: 0.0144\n",
      "Epoch [24/1], Step [53/63], Loss: 0.0468\n",
      "Epoch [24/1], Step [54/63], Loss: 0.0482\n",
      "Epoch [24/1], Step [55/63], Loss: 0.0445\n",
      "Epoch [24/1], Step [56/63], Loss: 0.0450\n",
      "Epoch [24/1], Step [57/63], Loss: 0.0777\n",
      "Epoch [24/1], Step [58/63], Loss: 0.0585\n",
      "Epoch [24/1], Step [59/63], Loss: 0.0261\n",
      "Epoch [24/1], Step [60/63], Loss: 0.0128\n",
      "Epoch [24/1], Step [61/63], Loss: 0.1689\n",
      "Epoch [24/1], Step [62/63], Loss: 0.0398\n",
      "Epoch [24/1], Step [63/63], Loss: 0.0377\n",
      "Accuracy: 0.1827511250536946\n",
      "Num Accuracy: 0.5346979530703944\n",
      "Epoch [25/1], Step [1/63], Loss: 0.0126\n",
      "Epoch [25/1], Step [2/63], Loss: 0.0336\n",
      "Epoch [25/1], Step [3/63], Loss: 0.0266\n",
      "Epoch [25/1], Step [4/63], Loss: 0.0333\n",
      "Epoch [25/1], Step [5/63], Loss: 0.0115\n",
      "Epoch [25/1], Step [6/63], Loss: 0.0368\n",
      "Epoch [25/1], Step [7/63], Loss: 0.0257\n",
      "Epoch [25/1], Step [8/63], Loss: 0.0173\n",
      "Epoch [25/1], Step [9/63], Loss: 0.0362\n",
      "Epoch [25/1], Step [10/63], Loss: 0.0215\n",
      "Epoch [25/1], Step [11/63], Loss: 0.0263\n",
      "Epoch [25/1], Step [12/63], Loss: 0.0277\n",
      "Epoch [25/1], Step [13/63], Loss: 0.0281\n",
      "Epoch [25/1], Step [14/63], Loss: 0.0416\n",
      "Epoch [25/1], Step [15/63], Loss: 0.0150\n",
      "Epoch [25/1], Step [16/63], Loss: 0.0197\n",
      "Epoch [25/1], Step [17/63], Loss: 0.0867\n",
      "Epoch [25/1], Step [18/63], Loss: 0.0368\n",
      "Epoch [25/1], Step [19/63], Loss: 0.0424\n",
      "Epoch [25/1], Step [20/63], Loss: 0.0520\n",
      "Epoch [25/1], Step [21/63], Loss: 0.0158\n",
      "Epoch [25/1], Step [22/63], Loss: 0.0520\n",
      "Epoch [25/1], Step [23/63], Loss: 0.0707\n",
      "Epoch [25/1], Step [24/63], Loss: 0.0355\n",
      "Epoch [25/1], Step [25/63], Loss: 0.0917\n",
      "Epoch [25/1], Step [26/63], Loss: 0.0553\n",
      "Epoch [25/1], Step [27/63], Loss: 0.0521\n",
      "Epoch [25/1], Step [28/63], Loss: 0.0322\n",
      "Epoch [25/1], Step [29/63], Loss: 0.0285\n",
      "Epoch [25/1], Step [30/63], Loss: 0.0791\n",
      "Epoch [25/1], Step [31/63], Loss: 0.0411\n",
      "Epoch [25/1], Step [32/63], Loss: 0.0324\n",
      "Epoch [25/1], Step [33/63], Loss: 0.0310\n",
      "Epoch [25/1], Step [34/63], Loss: 0.0329\n",
      "Epoch [25/1], Step [35/63], Loss: 0.0285\n",
      "Epoch [25/1], Step [36/63], Loss: 0.0073\n",
      "Epoch [25/1], Step [37/63], Loss: 0.0334\n",
      "Epoch [25/1], Step [38/63], Loss: 0.0251\n",
      "Epoch [25/1], Step [39/63], Loss: 0.0200\n",
      "Epoch [25/1], Step [40/63], Loss: 0.0285\n",
      "Epoch [25/1], Step [41/63], Loss: 0.0843\n",
      "Epoch [25/1], Step [42/63], Loss: 0.0307\n",
      "Epoch [25/1], Step [43/63], Loss: 0.0429\n",
      "Epoch [25/1], Step [44/63], Loss: 0.0593\n",
      "Epoch [25/1], Step [45/63], Loss: 0.0679\n",
      "Epoch [25/1], Step [46/63], Loss: 0.0593\n",
      "Epoch [25/1], Step [47/63], Loss: 0.0510\n",
      "Epoch [25/1], Step [48/63], Loss: 0.0187\n",
      "Epoch [25/1], Step [49/63], Loss: 0.0301\n",
      "Epoch [25/1], Step [50/63], Loss: 0.0379\n",
      "Epoch [25/1], Step [51/63], Loss: 0.0211\n",
      "Epoch [25/1], Step [52/63], Loss: 0.0484\n",
      "Epoch [25/1], Step [53/63], Loss: 0.0537\n",
      "Epoch [25/1], Step [54/63], Loss: 0.0254\n",
      "Epoch [25/1], Step [55/63], Loss: 0.0200\n",
      "Epoch [25/1], Step [56/63], Loss: 0.0368\n",
      "Epoch [25/1], Step [57/63], Loss: 0.0452\n",
      "Epoch [25/1], Step [58/63], Loss: 0.0587\n",
      "Epoch [25/1], Step [59/63], Loss: 0.0529\n",
      "Epoch [25/1], Step [60/63], Loss: 0.0385\n",
      "Epoch [25/1], Step [61/63], Loss: 0.0193\n",
      "Epoch [25/1], Step [62/63], Loss: 0.0519\n",
      "Epoch [25/1], Step [63/63], Loss: 0.0331\n",
      "Accuracy: 0.1964886330448199\n",
      "Num Accuracy: 0.5766350474288567\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_step = len(train_dataset)\n",
    "for epoch in range(25):\n",
    "    model.train()\n",
    "    for i, (X, Y, W) in enumerate(train_dataset):\n",
    "        # Forward pass\n",
    "        output = model(X)\n",
    "        Y = np.argmax(Y, axis=1)\n",
    "    \n",
    "        # W = W.reshape(len(X), 1)\n",
    "        criterion.weight = all_weights\n",
    "        loss = criterion(output, Y)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "\n",
    "    y_pred_list = np.empty(0)\n",
    "    y_target_list = np.empty(0)\n",
    "    y_weight_list = np.empty(0)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (X, Y, W) in enumerate(valid_dataset):\n",
    "            output = model(X)\n",
    "    \n",
    "            y_pred_list = np.concatenate((y_pred_list, (np.argmax(output, axis = 1))), axis = 0)\n",
    "    \n",
    "            y_target_list = np.concatenate((y_target_list, (np.argmax(Y, axis=1))), axis= 0)\n",
    "    \n",
    "            y_weight_list = np.concatenate((y_weight_list, W), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    conf_list.append(confusion_matrix(y_pred_list,y_target_list))\n",
    "    precise_list.append(precision_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    recall_list.append(recall_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    f1_list.append(f1_score(y_pred_list,y_target_list, average=\"weighted\"))\n",
    "    weightacc_list.append(weight_accuracy(y_pred_list,y_target_list, y_weight_list))\n",
    "    numacc_list.append(num_accuracy(y_pred_list,y_target_list))\n",
    "    \n",
    "    total_epochs += num_epochs\n",
    "    print(\"Accuracy:\", weight_accuracy(y_pred_list,y_target_list, y_weight_list))\n",
    "    print(\"Num Accuracy:\", num_accuracy(y_pred_list,y_target_list))\n",
    "    if (epoch+1 in [1, 3, 5, 10, 25]):\n",
    "        torch.save(model.state_dict(), \"C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_parameters_epoch{}.pth\".format(epoch+1))\n",
    "\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_conf_list.npy', np.array(conf_list))\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_precise_list.npy', np.array(precise_list))\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_recall_list.npy', np.array(recall_list))\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_f1_list.npy', np.array(f1_list))\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_weightacc_list.npy', np.array(weightacc_list))\n",
    "np.save('C:/Users/Derp/Documents/CS 184A BioAi/Project/HAM10000_img/cnn_numacc_list.npy', np.array(numacc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18334d-e0e7-4a58-99ff-f79673b06ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
